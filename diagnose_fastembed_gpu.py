#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¯Šæ–­ fastembed GPU ä½¿ç”¨æƒ…å†µ
"""

import sys
from pathlib import Path

# Windows æ§åˆ¶å°å…¼å®¹æ€§
if sys.platform == "win32":
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

print("=" * 70)
print("FastEmbed GPU è¯Šæ–­")
print("=" * 70)

# 1. æ£€æŸ¥ fastembed ç‰ˆæœ¬
print("\næ­¥éª¤ 1: æ£€æŸ¥ fastembed ç‰ˆæœ¬")
print("-" * 70)

try:
    import fastembed
    print(f"âœ… fastembed ç‰ˆæœ¬: {fastembed.__version__}")
    print(f"   æ¨¡å—è·¯å¾„: {fastembed.__file__}")
except ImportError as e:
    print(f"âŒ fastembed å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

# 2. æ£€æŸ¥ onnxruntime ç‰ˆæœ¬å’Œ CUDA æ”¯æŒ
print("\næ­¥éª¤ 2: æ£€æŸ¥ onnxruntime CUDA æ”¯æŒ")
print("-" * 70)

try:
    import onnxruntime as ort
    print(f"âœ… ONNX Runtime ç‰ˆæœ¬: {ort.__version__}")
    print(f"   æ¨¡å—è·¯å¾„: {ort.__file__}")

    # æ£€æŸ¥å¯ç”¨çš„ Execution Providers
    available_eps = ort.get_available_providers()
    print(f"\nå¯ç”¨çš„ Execution Providers:")
    for ep in available_eps:
        status = "âœ…" if "CUDA" in ep or "Tensorrt" in ep else "  "
        print(f"  {status} {ep}")

    if "CUDAExecutionProvider" in available_eps:
        print("\nâœ… CUDA Execution Provider å¯ç”¨!")
    else:
        print("\nâš ï¸  CUDA Execution Provider ä¸å¯ç”¨!")
        print("   å³ä½¿ä½¿ç”¨ fastembed-gpu,ä¹Ÿä¼šå›é€€åˆ° CPU")

except ImportError as e:
    print(f"âŒ onnxruntime å¯¼å…¥å¤±è´¥: {e}")

# 3. æ£€æŸ¥ PyTorch CUDA
print("\næ­¥éª¤ 3: æ£€æŸ¥ PyTorch CUDA æ”¯æŒ")
print("-" * 70)

try:
    import torch
    print(f"âœ… PyTorch ç‰ˆæœ¬: {torch.__version__}")
    print(f"   æ¨¡å—è·¯å¾„: {torch.__file__}")

    if torch.cuda.is_available():
        print(f"âœ… CUDA å¯ç”¨: {torch.version.cuda}")
        gpu_count = torch.cuda.device_count()
        print(f"   GPU æ•°é‡: {gpu_count}")

        for i in range(gpu_count):
            props = torch.cuda.get_device_properties(i)
            print(f"\n   GPU {i}: {props.name}")
            print(f"     æ˜¾å­˜: {props.total_memory / 1e9:.1f} GB")
            print(f"     è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
    else:
        print("âš ï¸  CUDA ä¸å¯ç”¨")

except ImportError:
    print("âš ï¸  PyTorch æœªå®‰è£…")

# 4. æµ‹è¯• fastembed TextEmbedding
print("\næ­¥éª¤ 4: æµ‹è¯• fastembed TextEmbedding")
print("-" * 70)

try:
    from fastembed import TextEmbedding

    print("åˆ›å»º TextEmbedding æ¨¡å‹...")

    # æµ‹è¯•ä¸æŒ‡å®š providers
    print("\næµ‹è¯• 1: é»˜è®¤é…ç½® (ä¸æŒ‡å®š providers)")
    model1 = TextEmbedding(model_name="BAAI/bge-small-en-v1.5")

    # æ£€æŸ¥æ¨¡å‹ä½¿ç”¨çš„ providers
    if hasattr(model1.model, 'get_providers'):
        providers = model1.model.get_providers()
        print(f"  ä½¿ç”¨çš„ Providers: {providers}")
    else:
        print("  âš ï¸  æ— æ³•è·å– providers ä¿¡æ¯ (æ¨¡å‹å¯èƒ½æœªåˆå§‹åŒ–)")

    # æµ‹è¯•æŒ‡å®š CUDAExecutionProvider
    print("\næµ‹è¯• 2: æ˜¾å¼æŒ‡å®š CUDAExecutionProvider")
    try:
        model2 = TextEmbedding(
            model_name="BAAI/bge-small-en-v1.5",
            providers=["CUDAExecutionProvider"]
        )

        if hasattr(model2.model, 'get_providers'):
            providers = model2.model.get_providers()
            print(f"  ä½¿ç”¨çš„ Providers: {providers}")
        else:
            print("  âš ï¸  æ— æ³•è·å– providers ä¿¡æ¯")

        # æµ‹è¯•å‘é‡åŒ–
        print("\n  æµ‹è¯•å‘é‡åŒ–...")
        test_texts = ["è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬", "FastEmbed GPU åŠ é€Ÿæµ‹è¯•"]

        import time
        start = time.time()
        embeddings = list(model2.embed(test_texts))
        elapsed = time.time() - start

        print(f"  âœ… å‘é‡åŒ–æˆåŠŸ! è€—æ—¶: {elapsed:.3f}s")
        print(f"     å‘é‡ç»´åº¦: {len(embeddings[0])}")

        # æ£€æŸ¥ GPU ä½¿ç”¨
        try:
            import subprocess
            result = subprocess.run(
                ['nvidia-smi', '--query-gpu=memory.used,utilization.gpu',
                 '--format=csv,noheader,nounits'],
                capture_output=True,
                text=True,
                timeout=5
            )
            if result.returncode == 0:
                parts = result.stdout.strip().split(', ')
                print(f"  GPU çŠ¶æ€: æ˜¾å­˜ {parts[0]}MB, åˆ©ç”¨ç‡ {parts[1]}%")
        except:
            pass

    except Exception as e:
        print(f"  âŒ æŒ‡å®š CUDAExecutionProvider å¤±è´¥: {e}")
        print("     è¿™é€šå¸¸æ„å‘³ç€ onnxruntime-gpu çš„ CUDA EP ä¸å¯ç”¨")

except Exception as e:
    print(f"âŒ fastembed æµ‹è¯•å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()

# 5. ç»“è®ºå’Œå»ºè®®
print("\n" + "=" * 70)
print("è¯Šæ–­ç»“è®º")
print("=" * 70)

print("""
æ ¹æ®æµ‹è¯•ç»“æœ,fastembed GPU æ”¯æŒæƒ…å†µ:

âœ… å¦‚æœ "CUDAExecutionProvider" åœ¨å¯ç”¨åˆ—è¡¨ä¸­:
   â†’ fastembed-gpu æ­£ç¡®å®‰è£…
   â†’ éœ€è¦åœ¨ä»£ç ä¸­æ˜¾å¼æŒ‡å®š providers=["CUDAExecutionProvider"]
   â†’ ä¿®æ”¹ qmd/llm/engine.py ç¬¬195-206è¡Œ

âš ï¸  å¦‚æœ "CUDAExecutionProvider" ä¸åœ¨å¯ç”¨åˆ—è¡¨ä¸­:
   â†’ onnxruntime-gpu çš„ CUDA æ”¯æŒæœªæ­£ç¡®é…ç½®
   â†’ å¯èƒ½åŸå› :
     * CUDA é©±åŠ¨ç‰ˆæœ¬ä¸åŒ¹é…
     * cuDNN æœªå®‰è£…æˆ–ç‰ˆæœ¬ä¸åŒ¹é…
     * onnxruntime-gpu ç‰ˆæœ¬ä¸ CUDA ç‰ˆæœ¬ä¸å…¼å®¹

ğŸ“ ä¸‹ä¸€æ­¥:
   1. æ£€æŸ¥ CUDA é©±åŠ¨: nvidia-smi
   2. æ£€æŸ¥ CUDA ç‰ˆæœ¬åŒ¹é…: pyproject.toml ä¸­çš„ç‰ˆæœ¬
   3. å¿…è¦æ—¶é‡æ–°å®‰è£… onnxruntime-gpu
""")
