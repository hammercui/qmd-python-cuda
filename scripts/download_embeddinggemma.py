#!/usr/bin/env python
"""
EmbeddingGemma-300M Q4F16 ONNX æ¨¡å‹ä¸‹è½½è„šæœ¬

ä» onnx-community/embeddinggemma-300m-ONNX ä¸‹è½½ Q4F16 å˜ä½“ï¼ˆ~176 MBï¼‰ã€‚
Q4 æƒé‡ + FP16 è®¡ç®—ï¼Œæ”¯æŒ CUDA GPU åŠ é€Ÿã€‚

ç”¨æ³•ï¼š
    python scripts/download_embeddinggemma.py           # ä¸‹è½½åˆ°é»˜è®¤è·¯å¾„
    python scripts/download_embeddinggemma.py --force   # å¼ºåˆ¶é‡æ–°ä¸‹è½½
    python scripts/download_embeddinggemma.py --check   # ä»…æ£€æµ‹ï¼Œä¸ä¸‹è½½
    python scripts/download_embeddinggemma.py --model-dir D:/models/embeddinggemma

å›½å†…åŠ é€Ÿï¼ˆHF é•œåƒï¼‰ï¼š
    set HF_ENDPOINT=https://hf-mirror.com
    python scripts/download_embeddinggemma.py
"""

import sys
import shutil
from pathlib import Path


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å¸¸é‡
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

REPO_ID    = "onnx-community/embeddinggemma-300m-ONNX"
MODEL_NAME = "google/embeddinggemma-300m"

# Q4F16ï¼šQ4 æƒé‡ + FP16 è®¡ç®—ï¼Œ~176 MBï¼ŒCUDA GPU å…¼å®¹
REQUIRED_FILES = [
    "onnx/model_q4f16.onnx",
    "onnx/model_q4f16.onnx_data",
    "tokenizer.json",
    "tokenizer_config.json",
    "config.json",
    "special_tokens_map.json",
]

DEFAULT_MODEL_DIR = Path.home() / ".cache" / "qmd" / "models" / "embeddinggemma-300m"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# è¾…åŠ©å‡½æ•°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _file_info(path: Path) -> str:
    size = path.stat().st_size
    if size >= 1024 * 1024:
        return f"{size / (1024 * 1024):.1f} MB"
    elif size >= 1024:
        return f"{size / 1024:.1f} KB"
    return f"{size} B"


def check_model(model_dir: Path) -> dict:
    """æ£€æµ‹ Q4F16 æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§ã€‚"""
    results = {"complete": True, "files": {}}
    for rel_path in REQUIRED_FILES:
        full_path = model_dir / rel_path
        if full_path.exists():
            results["files"][rel_path] = {"exists": True, "size": _file_info(full_path)}
        else:
            results["files"][rel_path] = {"exists": False, "size": "-"}
            results["complete"] = False
    return results


def print_check_result(model_dir: Path, result: dict) -> None:
    print(f"æ¨¡å‹ç›®å½• : {model_dir}")
    print(f"å˜ä½“     : Q4F16 (~176 MB, CUDA GPU)")
    print()
    print(f"{'æ–‡ä»¶':<45} {'çŠ¶æ€':<8} {'å¤§å°'}")
    print("-" * 70)
    for rel_path, info in result["files"].items():
        status = "âœ… å­˜åœ¨" if info["exists"] else "âŒ ç¼ºå¤±"
        print(f"  {rel_path:<43} {status:<8} {info['size']}")
    print()
    if result["complete"]:
        print("âœ… æ¨¡å‹å®Œæ•´ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ã€‚")
    else:
        print("âŒ æ¨¡å‹ä¸å®Œæ•´ï¼Œè¯·è¿è¡Œä¸‹è½½è„šæœ¬ã€‚")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ä¸‹è½½å‡½æ•°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def download_model(model_dir: Path = None, force: bool = False) -> bool:
    if model_dir is None:
        model_dir = DEFAULT_MODEL_DIR
    model_dir = Path(model_dir)
    model_dir.mkdir(parents=True, exist_ok=True)

    print("=" * 60)
    print("  EmbeddingGemma-300M Q4F16 ONNX ä¸‹è½½å™¨")
    print("=" * 60)
    print(f"æ¥æºä»“åº“ : {REPO_ID}")
    print(f"ç›®æ ‡ç›®å½• : {model_dir}")
    print()

    result = check_model(model_dir)
    if result["complete"] and not force:
        print("âœ… æ¨¡å‹å·²å­˜åœ¨ä¸”å®Œæ•´ï¼Œæ— éœ€é‡æ–°ä¸‹è½½ã€‚")
        print()
        print_check_result(model_dir, result)
        print()
        print("æç¤ºï¼šä½¿ç”¨ --force å¼ºåˆ¶é‡æ–°ä¸‹è½½ã€‚")
        return True

    if force and result["complete"]:
        print("âš ï¸  --force æ¨¡å¼ï¼šæ¸…é™¤æ—§ ONNX æ–‡ä»¶åé‡æ–°ä¸‹è½½...")
        onnx_dir = model_dir / "onnx"
        if onnx_dir.exists():
            shutil.rmtree(onnx_dir)
        print()

    try:
        from huggingface_hub import snapshot_download
    except ImportError:
        print("âŒ ç¼ºå°‘ huggingface_hubï¼špip install huggingface_hub")
        return False

    print("æ­£åœ¨ä¸‹è½½ä»¥ä¸‹æ–‡ä»¶ï¼š")
    for f in REQUIRED_FILES:
        print(f"  - {f}")
    print()
    print("â³ å¼€å§‹ä¸‹è½½ï¼ˆmodel_q4f16.onnx_data çº¦ 176 MBï¼Œè¯·è€å¿ƒç­‰å¾…ï¼‰...")
    print()

    try:
        snapshot_download(
            repo_id=REPO_ID,
            allow_patterns=REQUIRED_FILES,
            local_dir=str(model_dir),
            local_dir_use_symlinks=False,
        )
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥ï¼š{e}")
        print()
        print("è§£å†³æ–¹æ¡ˆï¼š")
        print("  å›½å†…é•œåƒï¼šset HF_ENDPOINT=https://hf-mirror.com")
        print("  ä»£    ç†ï¼šset HTTPS_PROXY=http://127.0.0.1:7890")
        import traceback
        traceback.print_exc()
        return False

    print()
    print("æ­£åœ¨éªŒè¯ä¸‹è½½çš„æ–‡ä»¶...")
    result = check_model(model_dir)
    print()
    print_check_result(model_dir, result)

    if not result["complete"]:
        print()
        print("âŒ éƒ¨åˆ†æ–‡ä»¶ç¼ºå¤±ï¼Œè¯·å°è¯• --force é‡æ–°ä¸‹è½½ã€‚")
        return False

    # å¿«é€Ÿæ¨ç†éªŒè¯
    print()
    print("æ­£åœ¨è¿›è¡Œæ¨ç†éªŒè¯ï¼ˆé€šè¿‡ fastembed åŠ è½½æ¨¡å‹ï¼‰...")
    try:
        from fastembed.text.text_embedding import TextEmbedding
        from fastembed.common.model_description import PoolingType, ModelSource

        TextEmbedding.add_custom_model(
            model=MODEL_NAME,
            pooling=PoolingType.MEAN,
            normalization=True,
            sources=ModelSource(hf=REPO_ID),
            dim=768,
            model_file="onnx/model_q4f16.onnx",
            additional_files=["onnx/model_q4f16.onnx_data"],
            description="EmbeddingGemma-300M Q4F16 ONNX, 768d",
            size_in_gb=0.18,
        )
        # ä¸æŒ‡å®š cache_dirï¼Œè®© fastembed ç®¡ç†è‡ªå·±çš„ç¼“å­˜ï¼ˆ~/.cache/fastembed/ï¼‰
        # é¦–æ¬¡åŠ è½½æ—¶ fastembed ä¼šå°†æ–‡ä»¶å¤åˆ¶åˆ°å…¶ç¼“å­˜ç›®å½•ï¼Œåç»­ç›´æ¥å¤ç”¨
        model = TextEmbedding(
            model_name=MODEL_NAME,
            providers=["CUDAExecutionProvider", "CPUExecutionProvider"],
        )
        embedding = list(model.embed(["title: none | text: Hello!"]))[0]
        assert len(embedding) == 768, f"ç»´åº¦é”™è¯¯ï¼š{len(embedding)}"
        print(f"âœ… æ¨ç†éªŒè¯é€šè¿‡ï¼ç»´åº¦ï¼š{len(embedding)}d")
    except ImportError:
        print("âš ï¸  fastembed æœªå®‰è£…ï¼Œè·³è¿‡éªŒè¯ã€‚")
    except Exception as e:
        print(f"âš ï¸  æ¨ç†éªŒè¯å¤±è´¥ï¼ˆæ–‡ä»¶å·²ä¸‹è½½ï¼‰ï¼š{e}")

    print()
    print("=" * 60)
    print("  ğŸ‰ ä¸‹è½½å®Œæˆï¼")
    print("=" * 60)
    print()
    print("åç»­æ­¥éª¤ï¼š")
    print("  qmd server   # å¯åŠ¨åµŒå…¥æœåŠ¡ï¼ˆè‡ªåŠ¨ä½¿ç”¨ CUDAï¼‰")
    print("  qmd embed    # ä¸ºæ–‡æ¡£ç”ŸæˆåµŒå…¥")
    print()
    return True


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CLI å…¥å£
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="ä¸‹è½½ EmbeddingGemma-300M Q4F16 ONNX æ¨¡å‹ï¼ˆCUDA GPU ç‰ˆæœ¬ï¼‰",
    )
    parser.add_argument("--check",     action="store_true", help="åªæ£€æµ‹ï¼Œä¸ä¸‹è½½")
    parser.add_argument("--force",     action="store_true", help="å¼ºåˆ¶é‡æ–°ä¸‹è½½")
    parser.add_argument("--model-dir", type=str, default=None,
                        help=f"è‡ªå®šä¹‰ä¿å­˜ç›®å½•ï¼ˆé»˜è®¤ï¼š{DEFAULT_MODEL_DIR}ï¼‰")

    args = parser.parse_args()
    target_dir = Path(args.model_dir) if args.model_dir else DEFAULT_MODEL_DIR

    if args.check:
        result = check_model(target_dir)
        print_check_result(target_dir, result)
        sys.exit(0 if result["complete"] else 1)

    success = download_model(model_dir=target_dir, force=args.force)
    sys.exit(0 if success else 1)


