#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¼•æ“æ€§èƒ½å¯¹æ¯”æµ‹è¯•
å¯¹æ¯” ONNX Runtime vs llama.cpp (CUDA) çš„æ€§èƒ½
"""

import sys
import time
from typing import List, Dict, Any

# Windows æ§åˆ¶å°å…¼å®¹æ€§
if sys.platform == "win32":
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')


def test_pytorch_reranker():
    """æµ‹è¯• PyTorch Reranker"""
    print("\n" + "=" * 70)
    print("æµ‹è¯• 1: PyTorch Reranker (cross-encoder)")
    print("=" * 70)

    try:
        from transformers import AutoTokenizer, AutoModelForSequenceClassification
        import torch

        model_name = "cross-encoder/ms-marco-MiniLM-L-6-v2"

        print(f"åŠ è½½æ¨¡å‹: {model_name}")
        start = time.time()
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSequenceClassification.from_pretrained(model_name)
        load_time = time.time() - start

        device = "cuda" if torch.cuda.is_available() else "cpu"
        model.to(device)
        model.eval()

        print(f"âœ“ æ¨¡å‹åŠ è½½å®Œæˆ ({load_time:.2f}s)")
        print(f"âœ“ è®¾å¤‡: {device}")

        # æµ‹è¯•æ¨ç†
        query = "What is machine learning?"
        docs = ["Machine learning is a subset of AI."] * 10

        start = time.time()
        with torch.no_grad():
            for doc in docs:
                inputs = tokenizer(query, doc, return_tensors="pt", truncation=True, max_length=512)
                inputs = {k: v.to(device) for k, v in inputs.items()}
                outputs = model(**inputs)
        inference_time = time.time() - start

        print(f"âœ“ æ¨ç†å®Œæˆ ({len(docs)} æ–‡æ¡£)")
        print(f"  æ¨ç†æ—¶é—´: {inference_time:.3f}s")
        print(f"  å¹³å‡æ¯æ–‡æ¡£: {inference_time / len(docs):.4f}s")

        # æ˜¾å­˜ä½¿ç”¨
        if torch.cuda.is_available():
            vram_mb = torch.cuda.max_memory_allocated() / 1024 / 1024
            print(f"  æ˜¾å­˜å ç”¨: {vram_mb:.0f}MB")

        return {
            "engine": "PyTorch",
            "load_time": load_time,
            "inference_time": inference_time,
            "per_doc": inference_time / len(docs),
            "vram_mb": vram_mb if torch.cuda.is_available() else 0
        }

    except Exception as e:
        print(f"âœ— æµ‹è¯•å¤±è´¥: {e}")
        return None


def test_onnx_reranker():
    """æµ‹è¯• ONNX Runtime Reranker"""
    print("\n" + "=" * 70)
    print("æµ‹è¯• 2: ONNX Runtime Reranker")
    print("=" * 70)

    try:
        from optimum.onnxruntime import ORTModelForSequenceClassification
        from transformers import AutoTokenizer
        import torch

        model_name = "XorPLM/ms-marco-MiniLM-L-6-v2-onnx"  # æˆ–å…¶ä»– ONNX ç‰ˆæœ¬

        print(f"åŠ è½½æ¨¡å‹: {model_name}")
        start = time.time()
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = ORTModelForSequenceClassification.from_pretrained(
            model_name,
            provider="CUDAExecutionProvider" if torch.cuda.is_available() else "CPUExecutionProvider"
        )
        load_time = time.time() - start

        print(f"âœ“ æ¨¡å‹åŠ è½½å®Œæˆ ({load_time:.2f}s)")

        # æµ‹è¯•æ¨ç†
        query = "What is machine learning?"
        docs = ["Machine learning is a subset of AI."] * 10

        start = time.time()
        for doc in docs:
            inputs = tokenizer(query, doc, return_tensors="pt", truncation=True, max_length=512)
            outputs = model(**inputs)
        inference_time = time.time() - start

        print(f"âœ“ æ¨ç†å®Œæˆ ({len(docs)} æ–‡æ¡£)")
        print(f"  æ¨ç†æ—¶é—´: {inference_time:.3f}s")
        print(f"  å¹³å‡æ¯æ–‡æ¡£: {inference_time / len(docs):.4f}s")

        return {
            "engine": "ONNX Runtime",
            "load_time": load_time,
            "inference_time": inference_time,
            "per_doc": inference_time / len(docs),
            "vram_mb": 0  # ONNX Runtime ä¸å®¹æ˜“è·å–æ˜¾å­˜
        }

    except Exception as e:
        print(f"âœ— æµ‹è¯•å¤±è´¥: {e}")
        return None


def test_llama_cpp_reranker():
    """æµ‹è¯• llama.cpp Reranker (GGUF)"""
    print("\n" + "=" * 70)
    print("æµ‹è¯• 3: llama.cpp Reranker (GGUF + CUDA)")
    print("=" * 70)

    try:
        from llama_cpp import Llama

        # æ³¨æ„ï¼šcross-encoder éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œè¿™é‡Œç”¨ LLM æ¨¡æ‹Ÿ
        # å®é™…ä¸­éœ€è¦æ‰¾åˆ° cross-encoder çš„ GGUF ç‰ˆæœ¬
        model_path = "~/.cache/qmd/models/Qwen2.5-0.5B-Instruct-Q4_K_M.gguf"

        print(f"åŠ è½½æ¨¡å‹: {model_path}")
        print("âš ï¸  æ³¨æ„: cross-encoder çš„ GGUF ç‰ˆæœ¬å¯èƒ½éœ€è¦è‡ªå·±è½¬æ¢")

        start = time.time()
        model = Llama(
            model_path=model_path,
            n_gpu_layers=-1,  # å…¨éƒ¨ GPU
            n_ctx=2048,
            verbose=False
        )
        load_time = time.time() - start

        print(f"âœ“ æ¨¡å‹åŠ è½½å®Œæˆ ({load_time:.2f}s")
        print(f"âœ“ GPU åŠ é€Ÿ: å·²å¯ç”¨")

        # æµ‹è¯•æ¨ç†ï¼ˆç”¨æ–‡æœ¬ç”Ÿæˆæ¨¡æ‹Ÿï¼‰
        query = "What is machine learning?"
        prompt = f"Query: {query}\nDocument: Machine learning is a subset of AI.\nRelevance:"

        start = time.time()
        for _ in range(10):
            output = model(prompt, max_tokens=1, temperature=0)
        inference_time = time.time() - start

        print(f"âœ“ æ¨ç†å®Œæˆ (10 æ¬¡)")
        print(f"  æ¨ç†æ—¶é—´: {inference_time:.3f}s")
        print(f"  å¹³å‡æ¯æ¬¡: {inference_time / 10:.4f}s")

        # æ˜¾å­˜ä¼°ç®—ï¼ˆGGUF Q4 å¤§çº¦æ˜¯åŸå¤§å°çš„ 1/4ï¼‰
        vram_mb = 80  # 110MB æ¨¡å‹ â†’ Q4 çº¦ 80MB
        print(f"  æ˜¾å­˜å ç”¨: ~{vram_mb}MB (ä¼°ç®—)")

        return {
            "engine": "llama.cpp (GGUF)",
            "load_time": load_time,
            "inference_time": inference_time,
            "per_doc": inference_time / 10,
            "vram_mb": vram_mb
        }

    except Exception as e:
        print(f"âœ— æµ‹è¯•å¤±è´¥: {e}")
        return None


def print_summary(results: List[Dict[str, Any]]):
    """æ‰“å°æ€§èƒ½å¯¹æ¯”æ€»ç»“"""
    print("\n" + "=" * 70)
    print("ğŸ“Š æ€§èƒ½å¯¹æ¯”æ€»ç»“")
    print("=" * 70)

    valid_results = [r for r in results if r is not None]

    if not valid_results:
        print("æ²¡æœ‰æœ‰æ•ˆçš„æµ‹è¯•ç»“æœ")
        return

    print(f"\n{'å¼•æ“':<20} {'åŠ è½½æ—¶é—´':<12} {'æ¨ç†æ—¶é—´':<12} {'æ¯æ–‡æ¡£':<12} {'æ˜¾å­˜':<10}")
    print("-" * 70)

    for r in valid_results:
        print(f"{r['engine']:<20} {r['load_time']:>8.2f}s   {r['inference_time']:>8.3f}s   "
              f"{r['per_doc']:>8.4f}s   {r['vram_mb']:>6.0f}MB")

    if len(valid_results) >= 2:
        print("\nğŸ“ˆ ç›¸å¯¹æå‡ (ä»¥ PyTorch ä¸ºåŸºå‡†):")
        baseline = valid_results[0]  # å‡è®¾ç¬¬ä¸€ä¸ªæ˜¯åŸºå‡†

        for r in valid_results[1:]:
            speedup = baseline['inference_time'] / r['inference_time']
            memory_reduction = (1 - r['vram_mb'] / baseline['vram_mb']) * 100

            print(f"\n{r['engine']}:")
            print(f"  é€Ÿåº¦æå‡: {speedup:.1f}x")
            print(f"  æ˜¾å­˜å‡å°‘: {memory_reduction:.0f}%")


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 70)
    print("ğŸ”¥ å¼•æ“æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    print("=" * 70)

    results = []

    # æµ‹è¯•å„ä¸ªå¼•æ“
    results.append(test_pytorch_reranker())
    # results.append(test_onnx_reranker())  # éœ€è¦å…ˆå®‰è£… optimum
    # results.append(test_llama_cpp_reranker())  # éœ€è¦å…ˆå®‰è£… llama-cpp-python

    # æ‰“å°æ€»ç»“
    print_summary(results)

    return 0


if __name__ == "__main__":
    sys.exit(main())
