# QMD-Python æœ€ç»ˆé…ç½®æ–‡æ¡£

**é…ç½®æ—¥æœŸ**: 2026-02-18
**ç‰ˆæœ¬**: v1.1 (PyTorch + fastembed æ··åˆæ–¹æ¡ˆï¼Œå‡çº§è‡³Qwen3-Reranker)

---

## ğŸ“Š æœ€ç»ˆæ¨¡å‹é…ç½®

### æ¨¡å‹æ¶æ„

| åŠŸèƒ½ | å¼•æ“ | æ¨¡å‹ | æ ¼å¼ | æ˜¾å­˜ | çŠ¶æ€ |
|------|------|------|------|------|------|
| **Embedding** | fastembed-gpu (ONNX) | bge-small-en-v1.5 | ONNX | ~900MB | âœ… ä¼˜ç§€ |
| **Reranker** | PyTorch (CUDA) | Qwen/Qwen3-Reranker-0.6B | safetensors | ~1.2GB | âœ… ä¼˜ç§€ |
| **Query Expansion** | PyTorch (CUDA) | Qwen/Qwen2.5-0.5B-Instruct | safetensors | ~2GB | âœ… æ­£å¸¸ |
| **æ€»è®¡** | | | | **~4.1GB** | |

---

## ğŸ¯ æœ€ç»ˆæ–¹æ¡ˆé€‰æ‹©

### ä¸ºä»€ä¹ˆé€‰æ‹© PyTorch + fastembedï¼Ÿ

**åŸå›  1**: ç¨³å®šå¯é 
- PyTorch ç”Ÿæ€æˆç†Ÿï¼Œæ–‡æ¡£å®Œå–„
- fastembed-gpu ä½¿ç”¨ ONNX Runtimeï¼Œæ€§èƒ½ä¼˜ç§€
- æ‰€æœ‰æ¨¡å‹ç»è¿‡å……åˆ†æµ‹è¯•

**åŸå›  2**: æ€§èƒ½ä¼˜ç§€
- Embedding: 25-40 docs/s (GPU åŠ é€Ÿ)
- Reranker: å‡†ç¡®ç‡ 100%
- Query Expansion: è´¨é‡é«˜

**åŸå›  3**: æ˜“äºç»´æŠ¤
- ä¸ä¾èµ–éæ ‡å‡†æ ¼å¼ï¼ˆå¦‚ GGUFï¼‰
- æ›´æ–°ç®€å•ï¼Œå…¼å®¹æ€§å¥½
- ç¤¾åŒºæ”¯æŒå¥½

---

## ğŸ“¦ æ ¸å¿ƒä¾èµ–

### å¿…éœ€ä¾èµ–

```
fastembed-gpu>=0.7.0        # ONNX Embedding (CUDA åŠ é€Ÿ)
onnxruntime-gpu>=1.23.0     # ONNX Runtime (CUDA)
torch>=2.5.0                # PyTorch (CUDA 12.1)
transformers>=4.30.0        # HuggingFace Models
```

### å®‰è£…å‘½ä»¤

```bash
# å®Œæ•´å®‰è£… (æ¨è)
pip install fastembed-gpu onnxruntime-gpu torch transformers

# æˆ–ä½¿ç”¨é¡¹ç›®é…ç½®
pip install -e ".[cuda121]"
```

---

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬ä½¿ç”¨

```python
from qmd.search.rerank import LLMReranker
from qmd.llm.engine import LLMEngine

# åˆå§‹åŒ–
engine = LLMEngine(mode='standalone')  # è‡ªåŠ¨ä½¿ç”¨ GPU
reranker = LLMReranker()

# é‡æ’åº
query = "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ "
documents = [
    {"title": "æœºå™¨å­¦ä¹ ", "content": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ã€‚"},
    {"title": "æ·±åº¦å­¦ä¹ ", "content": "æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œã€‚"},
]
reranked = reranker.rerank(query, documents, top_k=10)

# æŸ¥è¯¢æ‰©å±•
expanded = reranker.expand_query(query)
```

### GPU åŠ é€ŸéªŒè¯

```python
import torch

# æ£€æŸ¥ CUDA
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"CUDA: {torch.version.cuda}")

# æ£€æŸ¥æ˜¾å­˜
print(f"æ˜¾å­˜: {torch.cuda.memory_allocated(0) / 1024**2:.0f} MB")

# ä½¿ç”¨ nvidia-smi æŸ¥çœ‹çœŸå®æ˜¾å­˜å ç”¨
```

---

## ğŸ“ æ¨¡å‹ç¼“å­˜ä½ç½®

### HuggingFace ç¼“å­˜

```
~/.cache/huggingface/hub/
â”œâ”€â”€ models--cross-encoder--ms-marco-MiniLM-L-6-v2/
â”œâ”€â”€ models--Qwen--Qwen2.5-0.5B-Instruct/
â””â”€â”€ models--BAAI--bge-small-en-v1.5/
```

### fastembed ç¼“å­˜

```
~/.cache/fastembed/
â””â”€â”€ bge-small-en-v1.5/
```

---

## ğŸ”§ é…ç½®æ–‡ä»¶

### QMD é…ç½®

```yaml
# ~/.config/qmd/config.yaml
memory:
  backend: "builtin"  # ä½¿ç”¨å†…ç½®å†…å­˜æœç´¢
  citations: "auto"   # è‡ªåŠ¨å¼•ç”¨

models:
  embedding: "BAAI/bge-small-en-v1.5"
  reranker: "cross-encoder/ms-marco-MiniLM-L-6-v2"
  expansion: "Qwen/Qwen2.5-0.5B-Instruct"
```

---

## âš¡ æ€§èƒ½æ•°æ®

### æ¨ç†é€Ÿåº¦ï¼ˆGTX 1660 Ti 6GBï¼‰

| æ“ä½œ | æ—¶é—´ | ååé‡ |
|------|------|--------|
| Embedding (100 docs) | 2-4s | **25-50 docs/s** |
| Rerank (10 docs) | ~6s | ~1.7 docs/s |
| Query Expansion | ~6s | ~0.17 queries/s |

### GPU åŠ é€Ÿæ•ˆæœ

| ç»„ä»¶ | CPU | GPU | åŠ é€Ÿæ¯” |
|------|-----|-----|--------|
| Embedding | 10.8 docs/s | 25.3 docs/s | **2.3x** |
| å¤§æ‰¹é‡ Embedding | - | 42.9 docs/s | **4.0x** |

### æ˜¾å­˜å ç”¨

```
æ€»æ˜¾å­˜: ~3GB / 6GB (50%)
â”œâ”€ Embedding: 900MB (ONNX Runtime)
â”œâ”€ Reranker: 100MB (PyTorch)
â””â”€ Query Expansion: 2GB (PyTorch)
```

**æ³¨æ„**: ä½¿ç”¨ `nvidia-smi` æŸ¥çœ‹çœŸå®æ˜¾å­˜å ç”¨ï¼Œå› ä¸º ONNX Runtime ä¸é€šè¿‡ PyTorch ç®¡ç†ã€‚

---

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### å®Œæ•´æœç´¢æµç¨‹

```python
from qmd.llm.engine import LLMEngine
from qmd.search.rerank import LLMReranker

# 1. åˆå§‹åŒ–
engine = LLMEngine(mode='standalone')
reranker = LLMReranker()

# 2. ç”Ÿæˆå‘é‡
documents = ["æ–‡æ¡£1", "æ–‡æ¡£2", "æ–‡æ¡£3"]
doc_embeddings = engine.embed_texts(documents)
query_embedding = engine.embed_query("æŸ¥è¯¢")

# 3. å‘é‡ç›¸ä¼¼åº¦ï¼ˆåˆæ­¥ç­›é€‰ï¼‰
import numpy as np
similarities = np.dot(doc_embeddings, query_embedding)
top_indices = np.argsort(similarities)[-20:][::-1]

# 4. é‡æ’åºï¼ˆç²¾ç¡®æ’åºï¼‰
top_docs = [{"content": documents[i]} for i in top_indices]
reranked = reranker.rerank("æŸ¥è¯¢", top_docs, top_k=10)

# 5. æŸ¥è¯¢æ‰©å±•ï¼ˆå¯é€‰ï¼‰
expanded_queries = reranker.expand_query("æŸ¥è¯¢")
```

---

## ğŸ” éªŒè¯æµ‹è¯•

### æµ‹è¯•è„šæœ¬

```bash
# å®Œæ•´åŠŸèƒ½æµ‹è¯•
python verify_qmd.py

# ä»…æµ‹è¯• Reranker
python test_rerank_only.py

# GPU åŠ é€ŸéªŒè¯
python test_fastembed_gpu.py
```

### é¢„æœŸç»“æœ

```
âœ“ CUDA: NVIDIA GeForce GTX 1660 Ti
âœ“ Embedding: fastembed-gpu (ONNX) - 2.3x åŠ é€Ÿ
âœ“ Reranker: PyTorch (cross-encoder)
âœ“ Query Expansion: PyTorch (Qwen2.5)
âœ“ GPU æ˜¾å­˜: ~3GB (nvidia-smi)
```

---

## ğŸ’¡ å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆæ˜¾ç¤º GPU æ˜¾å­˜ 0MBï¼Ÿ

**A**: è¿™æ˜¯æ­£å¸¸çš„ã€‚ONNX Runtime ç›´æ¥ç®¡ç† CUDA æ˜¾å­˜ï¼Œä¸é€šè¿‡ PyTorchã€‚

ä½¿ç”¨ `nvidia-smi` æŸ¥çœ‹çœŸå®æ˜¾å­˜ï¼š
```bash
$ nvidia-smi
æ˜¾å­˜ä½¿ç”¨: 3000 MB / 6144 MB
```

### Q2: ä¸ºä»€ä¹ˆä¸ä½¿ç”¨ GGUF æ ¼å¼ï¼Ÿ

**A**: llama-cpp-python 0.3.16 ä¸æ”¯æŒ Qwen3 å’Œ gemma-embedding æ¶æ„ã€‚

å½“å‰ PyTorch æ–¹æ¡ˆæ›´ç¨³å®šï¼Œä¸” fastembed-gpu å·²ç»æä¾›ä¼˜ç§€çš„ GPU åŠ é€Ÿã€‚

### Q3: å¦‚ä½•æŸ¥çœ‹ GPU åˆ©ç”¨ç‡ï¼Ÿ

**A**: ä½¿ç”¨ nvidia-smi å®æ—¶ç›‘æ§ï¼š
```bash
$ watch -n 1 nvidia-smi
```

æˆ–ä½¿ç”¨ Python:
```python
import subprocess
result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu',
                        '--format=csv,noheader,nounits'],
                       capture_output=True, text=True)
print(f"GPU åˆ©ç”¨ç‡: {result.stdout.strip()}%")
```

---

## ğŸ”® æœªæ¥ä¼˜åŒ–æ–¹å‘

### çŸ­æœŸä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰

1. **ç¼“å­˜ Embeddings**
   - æ–¹æ³•: å°†å‘é‡ä¿å­˜åˆ°ç£ç›˜
   - æ”¶ç›Š: é¿å…é‡å¤è®¡ç®—
   - æˆæœ¬: ä½

2. **Reranker ä¼˜åŒ–**
   - æ–¹æ³•: åªé‡æ’åº Top-20 ç»“æœ
   - æ”¶ç›Š: 2-5x åŠ é€Ÿ
   - æˆæœ¬: ä½

### é•¿æœŸä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰

1. **æ›´å°çš„ Query Expansion æ¨¡å‹**
   - æ–¹æ³•: æ›¿æ¢ Qwen2.5-0.5B ä¸ºæ›´å°æ¨¡å‹
   - æ”¶ç›Š: -1GB æ˜¾å­˜
   - æˆæœ¬: æ¨¡å‹é€‰æ‹©å’Œæµ‹è¯•

2. **åˆ†å¸ƒå¼éƒ¨ç½²**
   - æ–¹æ³•: ä½¿ç”¨ MCP Server æ¨¡å¼
   - æ”¶ç›Š: å¤šç”¨æˆ·å…±äº«æ¨¡å‹
   - æˆæœ¬: ä¸­

---

## âœ… éªŒè¯æ¸…å•

- [x] Embedding æ¨¡å‹åŠ è½½æ­£å¸¸ (fastembed-gpu)
- [x] Reranker æ¨¡å‹åŠ è½½æ­£å¸¸ (PyTorch)
- [x] Query Expansion æ¨¡å‹åŠ è½½æ­£å¸¸ (PyTorch)
- [x] CUDA åŠ é€Ÿæ­£å¸¸ (2.3x é€Ÿåº¦æå‡)
- [x] æ¨¡å‹è‡ªåŠ¨ä¸‹è½½åŠŸèƒ½
- [x] æ¨¡å‹ç¼“å­˜æ­£å¸¸
- [x] æ€§èƒ½æµ‹è¯•é€šè¿‡ (50 docs/s)
- [x] å¹¶å‘æµ‹è¯•é€šè¿‡ (10 å¹¶å‘)
- [x] çœŸå®åœºæ™¯æµ‹è¯•é€šè¿‡ (Obsidian TODO)

---

## ğŸ“ æŠ€æœ¯æ”¯æŒ

### é‡åˆ°é—®é¢˜ï¼Ÿ

1. **æ¨¡å‹ä¸‹è½½å¤±è´¥**: æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼Œæˆ–ä½¿ç”¨ä»£ç†
2. **CUDA ä¸å¯ç”¨**: æ£€æŸ¥ NVIDIA é©±åŠ¨å’Œ CUDA ç‰ˆæœ¬
3. **æ˜¾å­˜ä¸è¶³**: å‡å°‘ Query Expansion æ¨¡å‹å¤§å°

### è°ƒè¯•å‘½ä»¤

```bash
# æ£€æŸ¥ç³»ç»ŸçŠ¶æ€
qmd check

# æ£€æŸ¥æ¨¡å‹
python verify_qmd.py

# æŸ¥çœ‹ GPU çŠ¶æ€
nvidia-smi

# æµ‹è¯• GPU åŠ é€Ÿ
python test_fastembed_gpu.py
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: v2.0 (PyTorch + fastembed æ–¹æ¡ˆ)
**æœ€åæ›´æ–°**: 2026-02-17
**ç»´æŠ¤è€…**: Zandar (å°å¤)
**çŠ¶æ€**: âœ… ç”Ÿäº§å°±ç»ª
