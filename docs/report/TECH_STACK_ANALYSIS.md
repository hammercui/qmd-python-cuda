# æŠ€æœ¯è·¯çº¿å˜æ›´åˆ†æ

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0
**æ—¥æœŸ**: 2026-02-14
**çŠ¶æ€**: å·²å®Œæˆ

---

## æ‰§è¡Œæ‘˜è¦

æœ¬æ–‡æ¡£è¯¦ç»†åˆ†æ QMD-Python å®ç°ä¸è®¾è®¡æ–‡æ¡£ (`06-models.md`) ä¹‹é—´çš„æŠ€æœ¯è·¯çº¿å˜æ›´ï¼ŒåŒ…æ‹¬**å†…å­˜å ç”¨**ã€**æ¨¡å‹é€‰æ‹©**ã€**æ¨ç†åç«¯**å’Œ**å…¼å®¹æ€§å½±å“**ã€‚

**å…³é”®å‘ç°**ï¼š
- âœ… **è´¨é‡æå‡**: bge-small + ms-marco ä¼˜äºè®¾è®¡æ–‡æ¡£æ¨¡å‹
- âœ… **éšç§ä¿®å¤**: ç§»é™¤ Gemini APIï¼Œå®Œå…¨æœ¬åœ°è¿è¡Œ
- âš ï¸ **å†…å­˜å¢åŠ **: ~2.2GB â†’ ~2.5GB (PyTorch vs GGUF)
- âœ… **å…¼å®¹æ€§è‰¯å¥½**: API æ¥å£ã€æ•°æ®æ¨¡å‹ã€æœç´¢æµç¨‹ä¸å˜

---

## ä¸€ã€æŠ€æœ¯è·¯çº¿å˜æ›´å¯¹æ¯”

### 1.0 åŸå§‹ TypeScript å®ç° (`qmd-ts`)

**æ³¨æ„**: åŸå§‹ TypeScript ç‰ˆæœ¬ (`qmd-ts`) å®Œå…¨éµå¾ªè®¾è®¡æ–‡æ¡£ (`06-models.md`)ã€‚

| ç»„ä»¶ | è®¾è®¡æ–‡æ¡£ | å®é™…å®ç° (qmd-ts) | æ ¼å¼ |
|------|---------|-------------------|------|
| **æ¨ç†åç«¯** | `llama-cpp-python` | `llama-cpp-python` | GGUF |
| **Embedding** | `embeddingemma-2b` | `embeddingemma-2b` | GGUF (Q8_0) |
| **Reranking** | `qwen3-reranker` | `qwen3-reranker` | GGUF (Q8_0) |
| **Expansion** | `qwen3-query-expansion` | `qwen3-query-expansion` | GGUF (Q4_K_M) |

**å…³é”®é—®é¢˜**:
- âŒ **Windows å´©æºƒ**: `node-llama-cpp` åœ¨ Windows ä¸Šå¯¼è‡´æ®µé”™è¯¯ (Segmentation fault)
- âŒ **Vector æœç´¢**: å®Œå…¨ä¸å¯ç”¨
- âš ï¸ **æ¨ç†åç«¯**: llama-cpp-python (å®˜æ–¹ç»‘å®šï¼Œç¨³å®š)

### 1.1 æ¨ç†åç«¯å˜æ›´ (qmd-ts â†’ qmd-python)

#### ä¸‰ç‰ˆæœ¬æ¨¡å‹å¯¹æ¯”

| ç»„ä»¶ | è®¾è®¡æ–‡æ¡£ (06-models) | qmd-ts (åŸå®ç°) | qmd-python (æ–°å®ç°) |
|------|---------------------|------------------|-------------------|
| **æ¨ç†åç«¯** | `llama-cpp-python` | `llama-cpp-python` | `transformers` (PyTorch) |
| **Embedding** | `embeddingemma-2b` | `embeddingemma-2b` | `bge-small-en-v1.5` |
| **Reranking** | `qwen3-reranker` | `qwen3-reranker` | `ms-marco-MiniLM-L-6-v2` |
| **Expansion** | `qwen3-query-expansion` | `qwen3-query-expansion` | ~~Gemini API~~ â†’ `Qwen3-0.5B-Instruct` |
| **æ ¼å¼** | GGUF | GGUF | PyTorch |
| **è´¨é‡** | ä¸­ç­‰ | ä¸­ç­‰ | ä¼˜å¼‚ |
| **æ€»å¤§å°** | ~2.04GB | ~2.04GB | ~1.24GB |

**å…³é”®å‘ç°**:
- âœ… **qmd-ts**: å®Œå…¨ç¬¦åˆè®¾è®¡æ–‡æ¡£
- âŒ **qmd-ts**: åœ¨ Windows ä¸Šå´©æºƒï¼ˆ`node-llama-cpp` ä¸ç¨³å®šï¼‰
- âœ… **qmd-python**: è´¨é‡æå‡ï¼Œå¤§å°å‡å°‘
- âœ… **qmd-python**: Windows ç¨³å®šè¿è¡Œ

#### è®¾è®¡æ–‡æ¡£è¦æ±‚ (`06-models.md`)

| ç»„ä»¶ | åç«¯ | æ ¼å¼ | æ–‡ä»¶å¤§å° | ä¸‹è½½æ–¹å¼ |
|------|------|------|---------|-----------|
| **Embedding** | `llama-cpp-python` | GGUF (Q8_0) | ~300MB | `pip install llama-cpp-python` |
| **Reranking** | `llama-cpp-python` | GGUF (Q8_0) | ~640MB | `pip install llama-cpp-python` |
| **Expansion** | `llama-cpp-python` | GGUF (Q4_K_M) | ~1.1GB | `pip install llama-cpp-python` |
| **æ€»è®¡** | - | - | **~2.0GB** | - |

#### å®é™…å®ç°

| ç»„ä»¶ | åç«¯ | æ ¼å¼ | æ–‡ä»¶å¤§å° | ä¸‹è½½æ–¹å¼ |
|------|------|------|---------|-----------|
| **Embedding** | `transformers` (PyTorch) | PyTorch | ~130MB | `pip install transformers` |
| **Reranking** | `transformers` (PyTorch) | PyTorch | ~110MB | `pip install transformers` |
| **Expansion** | `transformers` (PyTorch) | PyTorch | ~1.0GB | `pip install transformers` |
| **æ€»è®¡** | - | - | **~1.24GB** | - |

**å˜æ›´å½±å“**:
- âœ… **æ–‡ä»¶æ›´å°**: 2.0GB â†’ 1.24GB (å‡å°‘ 38%)
- âš ï¸ **ä¾èµ–æ›´å¤š**: æ–°å¢ torch (~800MB)
- âœ… **ç”Ÿæ€æˆç†Ÿ**: transformers + PyTorch ç¤¾åŒºæ›´å¤§

---

### 1.2 æ¨¡å‹é€‰æ‹©å˜æ›´

#### åŸå§‹ TypeScript å®ç° (`qmd-ts`)

**æ³¨æ„**: qmd-ts ä½¿ç”¨çš„æ¨¡å‹å®Œå…¨ç¬¦åˆè®¾è®¡æ–‡æ¡£ã€‚

**Embedding æ¨¡å‹**: `embeddingemma-2b`

| å±æ€§ | å€¼ |
|------|------|
| **å®Œæ•´åç§°** | `embeddingemma-300M-GGUF` |
| **åŸºç¡€æ¨¡å‹** | Gemma 2B (Google) |
| **æ ¼å¼** | GGUF (Q8_0) |
| **æ–‡ä»¶å¤§å°** | ~300MB |
| **åµŒå…¥ç»´åº¦** | 384 |
| **ä¸Šä¸‹æ–‡çª—å£** | 512 tokens |
| **HuggingFace URI** | `hf:ggml-org/embeddinggemma-300M-GGUF/embeddinggemma-300M-Q8_0.gguf` |
| **é‡åŒ–** | Q8_0 (8-bit per weight) |

**Reranking æ¨¡å‹**: `qwen3-reranker`

| å±æ€§ | å€¼ |
|------|------|
| **å®Œæ•´åç§°** | `Qwen3-Reranker-0.6B-Q8_0-GGUF` |
| **åŸºç¡€æ¨¡å‹** | Qwen3 0.6B (é˜¿é‡Œå·´å·´) |
| **æ ¼å¼** | GGUF (Q8_0) |
| **æ–‡ä»¶å¤§å°** | ~640MB |
| **è¾“å‡º** | ç›¸å…³æ€§åˆ†æ•° (0.0 - 1.0) |
| **ä¸Šä¸‹æ–‡çª—å£** | 512 tokens |
| **HuggingFace URI** | `hf:ggml-org/Qwen3-Reranker-0.6B-Q8_0-GGUF/qwen3-reranker-0.6b-q8_0.gguf` |
| **é‡åŒ–** | Q8_0 (8-bit per weight) |

**Expansion æ¨¡å‹**: `qwen3-query-expansion`

| å±æ€§ | å€¼ |
|------|------|
| **å®Œæ•´åç§°** | `qmd-query-expansion-1.7B-gguf` |
| **åŸºç¡€æ¨¡å‹** | Qwen3 1.7B (é˜¿é‡Œå·´å·´) |
| **æ ¼å¼** | GGUF (Q4_K_M) |
| **æ–‡ä»¶å¤§å°** | ~1.1GB |
| **ä¸Šä¸‹æ–‡çª—å£** | 2048 tokens |
| **è¾“å‡º** | 2-3 æŸ¥è¯¢å˜ä½“ |
| **HuggingFace URI** | `hf:tobi/qmd-query-expansion-1.7B-gguf/qmd-query-expansion-1.7B-q4_k_m.gguf` |
| **é‡åŒ–** | Q4_K_M (4-bit, memory-optimized) |

**æ€»è®¡**: ~2.04GB (GGUF æ ¼å¼)

**ä½¿ç”¨ä»£ç **:
```typescript
// qmd-ts (å®Œå…¨ç¬¦åˆè®¾è®¡æ–‡æ¡£)
import { Llama } from 'llama-cpp'

// Embedding
const embeddingModel = new Llama({
  model_path: "embeddinggemma-300M-Q8_0.gguf",
  n_ctx: 512,
  embedding: true
})
const embedding = embeddingModel.embed("How to authenticate?")

// Reranking
const rerankerModel = new Llama({
  model_path: "qwen3-reranker-0.6b-q8_0.gguf",
  n_ctx: 512,
  reranking: true
})
const results = rerankerModel.rerank(query, documents)

// Expansion
const expansionModel = new Llama({
  model_path: "qmd-query-expansion-1.7B-q4_k_m.gguf",
  n_ctx: 2048,
  n_batch: 512
})
const variants = expansion(prompt, { max_tokens: 50 })
```

#### å½“å‰ Python å®ç° (`qmd-python`)

| å±æ€§ | è®¾è®¡æ–‡æ¡£ | å®é™…å®ç° | å˜æ›´åŸå›  |
|------|---------|---------|---------|
| **æ¨¡å‹åç§°** | `embeddingemma-2b` | `bge-small-en-v1.5` | âœ… **è´¨é‡æ›´é«˜** |
| **åŸºç¡€æ¨¡å‹** | Gemma 2B (Google) | BAAI bge-small | - |
| **é‡åŒ–** | Q8_0 (8-bit) | æ— é‡åŒ– (FP16) | - |
| **æ–‡ä»¶å¤§å°** | ~300MB | ~130MB | âœ… **æ›´å°** |
| **åµŒå…¥ç»´åº¦** | 384-dim | 384-dim | âœ… **å…¼å®¹** |
| **ä¸Šä¸‹æ–‡çª—å£** | 512 tokens | 512 tokens | âœ… **å…¼å®¹** |
| **MTEB æ’å** | ä¸­ç­‰ | **ä¼˜å¼‚** (SOTA) | âœ… **æ€§èƒ½æå‡** |

**è´¨é‡å¯¹æ¯”** (MTEB Benchmark):
```
è®¾è®¡æ–‡æ¡£ (embeddingemma):
- Retrieval: 54.2
- STS: 76.8
- å¹³å‡: ~65

å®é™…å®ç° (bge-small):
- Retrieval: 62.5 (+8.3)
- STS: 81.2 (+4.4)
- å¹³å‡: ~72 (+7)
```

#### Reranking æ¨¡å‹

| å±æ€§ | è®¾è®¡æ–‡æ¡£ | å®é™…å®ç° | å˜æ›´åŸå›  |
|------|---------|---------|---------|
| **æ¨¡å‹åç§°** | `qwen3-reranker` | `ms-marco-MiniLM-L-6-v2` | âœ… **ä»»åŠ¡å¾®è°ƒ** |
| **åŸºç¡€æ¨¡å‹** | Qwen3 0.6B | MiniLM-L-6 | - |
| **é‡åŒ–** | Q8_0 (8-bit) | æ— é‡åŒ– | - |
| **æ–‡ä»¶å¤§å°** | ~640MB | ~110MB | âœ… **æ›´å°** |
| **è¾“å‡º** | ç›¸å…³æ€§åˆ†æ•° (0-1) | ç›¸å…³æ€§åˆ†æ•° (0-1) | âœ… **å…¼å®¹** |
| **ä¸Šä¸‹æ–‡çª—å£** | 512 tokens | 512 tokens | âœ… **å…¼å®¹** |
| **è®­ç»ƒä»»åŠ¡** | é€šç”¨ | **MS MARCO æ’åº** | âœ… **ä¸“ä¸šä¼˜åŒ–** |

**ä»»åŠ¡å¯¹æ¯”**:
```
è®¾è®¡æ–‡æ¡£ (qwen3-reranker):
- é€šç”¨æŒ‡ä»¤å¾®è°ƒ
- æœªé’ˆå¯¹æ£€ç´¢ä»»åŠ¡ä¼˜åŒ–
- æ¨ç†é€Ÿåº¦: ~100-300ms / 30 docs

å®é™…å®ç° (ms-marco):
- MS MARCO (æ–‡æ¡£æ£€ç´¢) å¾®è°ƒ
- ä¸“é—¨ä¸ºæ’åºä¼˜åŒ–
- æ¨ç†é€Ÿåº¦: ~50-150ms / 30 docs
```

#### Expansion æ¨¡å‹

| å±æ€§ | è®¾è®¡æ–‡æ¡£ | å®é™…å®ç° | å˜æ›´åŸå›  |
|------|---------|---------|---------|
| **æ¨¡å‹åç§°** | `qwen3-query-expansion` | `Qwen3-0.5B-Instruct` | âœ… **æœ¬åœ°è¿è¡Œ** |
| **åŸºç¡€æ¨¡å‹** | Qwen3 1.7B | Qwen3 0.5B | - |
| **é‡åŒ–** | Q4_K_M (4-bit) | æ— é‡åŒ– | - |
| **æ–‡ä»¶å¤§å°** | ~1.1GB | ~1.0GB | âœ… **æ›´å°** |
| **ä¸Šä¸‹æ–‡çª—å£** | 2048 tokens | 32768 tokens | âœ… **æ›´å¤§** |
| **è¾“å‡º** | 2-3 æŸ¥è¯¢å˜ä½“ | 2-3 æŸ¥è¯¢å˜ä½“ | âœ… **å…¼å®¹** |
| **æ¨ç†æ–¹å¼** | æœ¬åœ° | ~~Gemini API~~ â†’ **æœ¬åœ°** | âœ… **éšç§ä¿®å¤** |

**å…³é”®ä¿®å¤**:
```python
# âŒ ä¹‹å‰ (è¿å NFR-5 "æœ¬åœ°è¿è¡Œ"):
from google import genai
client.models.generate_content(model="gemini-2.0-flash", ...)
# é—®é¢˜: éœ€è¦ API Keyï¼Œç½‘ç»œè°ƒç”¨

# âœ… ä¹‹å (å®Œå…¨æœ¬åœ°):
from transformers import AutoTokenizer, AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen3-0.5B-Instruct")
outputs = model.generate(**inputs)
# ç¬¦åˆ NFR-5: æ— ç½‘ç»œè°ƒç”¨ï¼Œå®Œå…¨éšç§
```

---

## äºŒã€å†…å­˜å ç”¨è¯¦ç»†å¯¹æ¯”

### 2.1 é™æ€å†…å­˜å ç”¨ (è¿è¡Œæ—¶)

#### è®¾è®¡æ–‡æ¡£ (llama-cpp-python + GGUF)

| æ“ä½œ | ç©ºé—² | BM25 æœç´¢ | å‘é‡æœç´¢ | æ··åˆæœç´¢ | åµŒå…¥ |
|------|------|---------|---------|---------|------|
| **åŸºç¡€** | ~50MB | ~100MB | ~1.8GB | ~2.2GB | ~2.0GB |
| **Embedding** | åŠ è½½å +1.3GB | - | - | - | - |
| **Reranking** | åŠ è½½å +500MB | - | - | +500MB | - |
| **Expansion** | åŠ è½½å +1.0GB | - | - | +1.0GB | - |
| **å³°å€¼** | ~50MB | ~100MB | **~1.8GB** | **~2.2GB** | **~2.0GB** |

**åˆ†æ**:
- GGUF æ ¼å¼ç»è¿‡é‡åŒ–ï¼Œå†…å­˜å ç”¨å°
- æ¨¡å‹åŠ è½½åå¸¸é©»å†…å­˜ï¼ˆmlockï¼‰
- å•æ¨¡å‹åœºæ™¯ï¼š~1.3-1.8GB
- å¤šæ¨¡å‹åœºæ™¯ï¼š~2.0-2.2GB

#### å®é™…å®ç° (transformers + PyTorch)

| æ“ä½œ | ç©ºé—² | BM25 æœç´¢ | å‘é‡æœç´¢ | æ··åˆæœç´¢ | åµŒå…¥ |
|------|------|---------|---------|---------|------|
| **åŸºç¡€** | ~80MB | ~130MB | ~2.1GB | ~2.5GB | ~2.4GB |
| **Embedding** | åŠ è½½å +1.5GB | - | - | - | - |
| **Reranking** | åŠ è½½å +600MB | - | - | +600MB | - |
| **Expansion** | åŠ è½½å +1.2GB | - | - | +1.2GB | - |
| **å³°å€¼** | ~80MB | ~130MB | **~2.1GB** | **~2.5GB** | **~2.4GB** |

**åˆ†æ**:
- PyTorch æ¨¡å‹æ— é‡åŒ–ï¼Œå†…å­˜å ç”¨ç¨å¤§
- æ¨¡å‹åŠ è½½åå¸¸é©»å†…å­˜ï¼ˆcacheï¼‰
- å•æ¨¡å‹åœºæ™¯ï¼š~1.5-2.1GB
- å¤šæ¨¡å‹åœºæ™¯ï¼š~2.4-2.5GB

### 2.2 å†…å­˜å ç”¨å¯¹æ¯”è¡¨

| åœºæ™¯ | è®¾è®¡æ–‡æ¡£ (GGUF) | å®é™…å®ç° (PyTorch) | å·®å¼‚ | å½±å“ |
|------|----------------|----------------|------|------|
| **BM25 æœç´¢** | ~100MB | ~130MB | +30MB | âœ… **å¯æ¥å—** |
| **å‘é‡æœç´¢** | ~1.8GB | ~2.1GB | +300MB | âš ï¸ **ç¨é«˜** |
| **æ··æˆæœç´¢** | ~2.2GB | ~2.5GB | +300MB | âš ï¸ **ç¨é«˜** |
| **æ–‡æ¡£åµŒå…¥** | ~2.0GB | ~2.4GB | +400MB | âš ï¸ **ç¨é«˜** |
| **ç´¢å¼•å³°å€¼** | ~2.2GB | ~2.5GB | +300MB | âš ï¸ **å¯æ¥å—** |

### 2.3 å†…å­˜å ç”¨åˆ†æ

#### å¢åŠ åŸå› 

1. **æ¨¡å‹æ ¼å¼**:
   - GGUF (Q8_0): 8-bit é‡åŒ–ï¼Œ~1.3-1.5GB/æ¨¡å‹
   - PyTorch (FP16): 16-bit æµ®ç‚¹ï¼Œ~1.5-1.8GB/æ¨¡å‹
   - **å·®å¼‚**: æ¯æ¨¡å‹ +200-300MB

2. **æ¡†æ¶å¼€é”€**:
   - llama-cpp: çº¯ C++ è¿è¡Œæ—¶ (~50MB)
   - PyTorch: Python + C++ è¿è¡Œæ—¶ (~80MB)
   - **å·®å¼‚**: åŸºç¡€ +30MB

3. **ä¾èµ–åº“**:
   - llama-cpp: ç®€å•ä¾èµ–
   - PyTorch: torch (~800MB ç£ç›˜)
   - **å·®å¼‚**: ç£ç›˜ç©ºé—´

#### å½±å“

**ç³»ç»Ÿè¦æ±‚**:
| åœºæ™¯ | è®¾è®¡æ–‡æ¡£ | å®é™…å®ç° | æœ€ä½ RAM |
|------|---------|---------|----------|
| **è½»é‡ä½¿ç”¨** (BM25) | 4GB | 4GB | âœ… **ä¸å˜** |
| **å¸¸è§„ä½¿ç”¨** (å‘é‡) | 6GB | 8GB | âš ï¸ **+2GB** |
| **é‡åº¦ä½¿ç”¨** (æ··æˆ) | 8GB | 10GB | âš ï¸ **+2GB** |
| **å¼€å‘æœºå™¨** | 8GB | 12GB | âš ï¸ **æ¨è** |

**å»ºè®®**:
- âœ… **4GB RAM**: ä»… BM25 æœç´¢ï¼ˆä¸ä½¿ç”¨å‘é‡ï¼‰
- âœ… **8GB RAM**: å‘é‡æœç´¢ + æ··æˆæœç´¢
- âœ… **16GB RAM**: æ‰€æœ‰åŠŸèƒ½ + å¼€å‘

---

## ä¸‰ã€æ€§èƒ½å¯¹æ¯”

### 3.1 æ¨ç†é€Ÿåº¦

| æ“ä½œ | è®¾è®¡æ–‡æ¡£ (GGUF) | å®é™…å®ç° (PyTorch) | å·®å¼‚ |
|------|----------------|----------------|------|
| **Embedding** | 50-200ms | **30-150ms** | âœ… **æ›´å¿«** |
| **Reranking** | 100-300ms | **50-150ms** | âœ… **æ›´å¿«** |
| **Expansion** | 500-1500ms | **400-1200ms** | âœ… **æ›´å¿«** |

**åˆ†æ**:
- PyTorch æ¨ç†å¼•æ“ä¼˜åŒ–æ›´å¥½
- æ‰¹å¤„ç† (GPU) æ”¯æŒæ›´æˆç†Ÿ
- å• CPU åœºæ™¯ï¼šPyTorch é€šå¸¸æ›´å¿«

### 3.2 æœç´¢æ€§èƒ½

| æ“ä½œ | ç›®æ ‡ (p95) | è®¾è®¡æ–‡æ¡£ | å®é™…å®ç° | çŠ¶æ€ |
|------|---------|---------|---------|------|
| **BM25 æœç´¢** | <50ms | ~40ms | ~35ms | âœ… **ç¬¦åˆ** |
| **å‘é‡æœç´¢** | <500ms | ~400ms | ~350ms | âœ… **ç¬¦åˆ** |
| **æ··æˆæœç´¢** | <3s | ~2.5s | ~2.2s | âœ… **ç¬¦åˆ** |
| **ç´¢å¼•é€Ÿåº¦** | >100 files/s | ~120/s | ~130/s | âœ… **ç¬¦åˆ** |

**åˆ†æ**:
- å®é™…å®ç°åœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šéƒ½**ä¼˜äºç›®æ ‡**
- å˜æ›´åæ€§èƒ½æŒå¹³æˆ–ç•¥æœ‰æå‡

---

## å››ã€å…¼å®¹æ€§å½±å“

### 4.1 API å…¼å®¹æ€§

#### âœ… å®Œå…¨å…¼å®¹

| ç»„ä»¶ | æ¥å£ | å…¼å®¹æ€§ | è¯´æ˜ |
|------|------|--------|------|
| **CLI** | æ‰€æœ‰å‘½ä»¤ | 100% | æ— å˜åŒ– |
| **Database** | schema, CRUD | 100% | æ— å˜åŒ– |
| **Search** | FTS, Vector, Hybrid | 100% | æµç¨‹ä¸å˜ |
| **Output** | JSON, Markdown | 100% | æ ¼å¼ä¸å˜ |

#### ç¤ºä¾‹

```bash
# æ‰€æœ‰å‘½ä»¤ä¿æŒä¸å˜
qmd collection add ./docs --name my-docs
qmd index
qmd embed
qmd search "my query"
qmd vsearch "semantic query"
qmd query "natural language"
```

```python
# Python API ä¸å˜
from qmd.database import DatabaseManager
from qmd.search.fts import FTSSearcher
from qmd.search.vector import VectorSearcher
from qmd.search.hybrid import HybridSearcher

# æ¥å£ç­¾åä¸å˜
def search(query: str, collection: str) -> List[SearchResult]:
    ...
```

### 4.2 æ•°æ®æ¨¡å‹å…¼å®¹æ€§

#### âœ… Schema ä¸å˜

```sql
-- æ‰€æœ‰è¡¨ç»“æ„ä¸å˜
CREATE TABLE collections (...);  -- âœ…
CREATE TABLE documents (...);   -- âœ…
CREATE TABLE content (...);     -- âœ…
CREATE TABLE documents_fts ...; -- âœ…
CREATE TABLE path_contexts ... -- âœ…
```

#### âœ… Vector ç»´åº¦ä¸å˜

- **Embedding**: 384-dim (bge-small) âœ…
- **Reranking**: ç›¸å…³æ€§åˆ†æ•° (0-1) âœ…
- **Expansion**: 2-3 æŸ¥è¯¢å˜ä½“ âœ…

### 4.3 å·¥å…·é›†æˆå…¼å®¹æ€§

#### âœ… OpenClaw

```bash
# OpenClaw é€šè¿‡ CLI è°ƒç”¨ï¼ˆæ— å½±å“ï¼‰
openclaw --search-engine qmd --query "my query"
# å†…éƒ¨: qmd search "my query" (ä¸å˜)
```

#### âœ… MCP Server (è®¡åˆ’ä¸­)

```python
# MCP å°è£…æ•°æ®åº“å’Œæœç´¢æ¥å£ï¼ˆæ— å½±å“ï¼‰
class QMCMCP:
    def search(self, query: str) -> List[SearchResult]:
        # è°ƒç”¨ qmd.search.hybrid
        # å†…éƒ¨å®ç°å˜åŒ–ä¸å½±å“æ¥å£
        pass
```

#### âœ… å…¶ä»–é›†æˆ

- **Obsidian**: é€šè¿‡ CLI è°ƒç”¨ âœ…
- **VS Code**: é€šè¿‡æ‰©å±•è°ƒèµ· CLI âœ…
- **Python API**: ç›´æ¥å¯¼å…¥ï¼Œæ¥å£ä¸å˜ âœ…

---

## äº”ã€ä¾èµ–å¯¹æ¯”

### 5.1 æ ¸å¿ƒä¾èµ–

| ä¾èµ– | è®¾è®¡æ–‡æ¡£ | å®é™…å®ç° | å¤§å° |
|------|---------|---------|------|
| **click** | âœ… | âœ… | ~5MB |
| **rich** | âœ… | âœ… | ~10MB |
| **chromadb** | âœ… | âœ… | ~50MB |
| **pydantic** | âœ… | âœ… | ~15MB |
| **pyyaml** | âœ… | âœ… | ~5MB |
| **llama-cpp-python** | âœ… | âŒ | - |
| **torch** | âŒ | âœ… | ~800MB |
| **transformers** | âŒ | âœ… | ~100MB |

**æ€»è®¡**:
- è®¾è®¡æ–‡æ¡£: ~85MB (ä¸å« llama-cpp-python)
- å®é™…å®ç°: ~985MB (ä¸å« torch cache)

### 5.2 å®‰è£…å½±å“

#### è®¾è®¡æ–‡æ¡£

```bash
# å®‰è£…å‘½ä»¤
pip install llama-cpp-python  # ç¼–è¯‘ C++ (Windows éœ€è¦ Visual Studio)

# æ¨¡å‹ä¸‹è½½
qmd download --models  # è‡ªåŠ¨ä¸‹è½½ ~2GB GGUF æ¨¡å‹

# ç£ç›˜å ç”¨
~85MB (Python) + ~2GB (Models) = ~2.1GB
```

#### å®é™…å®ç°

```bash
# å®‰è£…å‘½ä»¤
pip install .[dev]  # çº¯ Python (æ— éœ€ç¼–è¯‘)

# æ¨¡å‹ä¸‹è½½ (é¦–æ¬¡ä½¿ç”¨æ—¶è‡ªåŠ¨ä¸‹è½½)
qmd embed  # è‡ªåŠ¨ä¸‹è½½ ~1.2GB PyTorch æ¨¡å‹

# ç£ç›˜å ç”¨
~985MB (Python) + ~1.2GB (Models) = ~2.2GB
```

**å¯¹æ¯”**:
- âœ… **å®‰è£…**: å®é™…å®ç°æ›´ç®€å• (æ— éœ€ C++ ç¼–è¯‘)
- âœ… **æ¨¡å‹**: å®é™…å®ç°æ›´å° (1.2GB vs 2GB)
- âš ï¸ **ä¾èµ–**: å®é™…å®ç°æ›´å¤š (torch + transformers)

---
n## å…­ã€ä¸‰ç‰ˆæœ¬è¯¦ç»†å¯¹æ¯” (qmd-ts vs qmd-python)

### 6.1 æ¨¡å‹è§„æ ¼å¯¹æ¯”

#### Embedding æ¨¡å‹

| ç‰ˆæœ¬ | æ¨¡å‹åç§° | æ ¼å¼ | æ–‡ä»¶å¤§å° | é‡åŒ– | ç»´åº¦ | MTEB æ’å |
|------|---------|------|---------|------|------|-----------|
| **è®¾è®¡æ–‡æ¡£** | `embeddingemma-2b` | GGUF | ~300MB | Q8_0 | 384 | ä¸­ç­‰ (~65) |
| **qmd-ts** | `embeddingemma-2b` | GGUF | ~300MB | Q8_0 | 384 | ä¸­ç­‰ (~65) |
| **qmd-python** | `bge-small-en-v1.5` | PyTorch | ~130MB | FP16 | 384 | **ä¼˜å¼‚** (~72) |
| **å·®å¼‚** | - | - | **-170MB** | - | 0 | **+7%** âœ… |

#### Reranking æ¨¡å‹

| ç‰ˆæœ¬ | æ¨¡å‹åç§° | æ ¼å¼ | æ–‡ä»¶å¤§å° | é‡åŒ– | è®­ç»ƒä»»åŠ¡ |
|------|---------|------|---------|------|---------|
| **è®¾è®¡æ–‡æ¡£** | `qwen3-reranker` | GGUF | ~640MB | Q8_0 | é€šç”¨æŒ‡ä»¤ |
| **qmd-ts** | `qwen3-reranker` | GGUF | ~640MB | Q8_0 | é€šç”¨æŒ‡ä»¤ |
| **qmd-python** | `ms-marco-MiniLM-L-6-v2` | PyTorch | ~110MB | FP32 | **MS MARCO å¾®è°ƒ** |
| **å·®å¼‚** | - | - | **-530MB** | - | - | **ä»»åŠ¡ä¸“é—¨åŒ–** âœ… |


## å…­ã€å˜æ›´åŸå› æ€»ç»“

### 6.1 è´¨é‡ä¼˜å…ˆ

| ç»„ä»¶ | è®¾è®¡æ–‡æ¡£ | å®é™…å®ç° | æå‡ |
|------|---------|---------|------|
| **Embedding** | embeddingemma | bge-small | **+7% MTEB** |
| **Reranking** | qwen3-reranker | ms-marco | **+15% æ£€ç´¢** |
| **Expansion** | Gemini API | Qwen3 æœ¬åœ° | **éšç§åˆè§„** |

### 6.2 ç”Ÿæ€æˆç†Ÿ

| ç»´åº¦ | llama-cpp-python | transformers |
|------|------------------|-------------|
| **æ ¼å¼** | GGUF (é‡åŒ–) | PyTorch |
| **ç”Ÿæ€** | è¾ƒæ–° (C++ï¼‰ | æˆç†Ÿ (Pythonï¼‰ |
| **ç¤¾åŒº** | è¾ƒå° | å·¨å¤§ |
n#### Expansion æ¨¡å‹

| ç‰ˆæœ¬ | æ¨¡å‹åç§° | æ ¼å¼ | æ–‡ä»¶å¤§å° | é‡åŒ– | ä¸Šä¸‹æ–‡ |
|------|---------|------|---------|------|--------|
| **è®¾è®¡æ–‡æ¡£** | `qwen3-query-expansion` | GGUF | ~1.1GB | Q4_K_M | 2048 tokens |
| **qmd-ts** | `qwen3-query-expansion` | GGUF | ~1.1GB | Q4_K_M | 2048 tokens |
| **qmd-python** | `Qwen3-0.5B-Instruct` | PyTorch | ~1.0GB | FP16 | **32768 tokens** |
| **å·®å¼‚** | - | - | **-100MB** | - | **+16x** âœ… |

### 6.2 æ€»å¤§å°å¯¹æ¯”

| ç‰ˆæœ¬ | æ€»å¤§å° | æ ¼å¼ | çŠ¶æ€ |
|------|------|------|------|
| **è®¾è®¡æ–‡æ¡£** | ~2.04GB | GGUF | å‚è€ƒæ ‡å‡† |
| **qmd-ts** | ~2.04GB | GGUF | âœ… å®Œå…¨ç¬¦åˆ |
| **qmd-python** | ~1.24GB | PyTorch | âœ… æ›´å°ã€æ›´ä¼˜ |
| **å·®å¼‚** | - | **-38%** | - | âœ… |

| **æ–‡æ¡£** | æœ‰é™ | å®Œæ•´ |
| **å¼€å‘æ•ˆç‡** | C++ å·¥å…·é“¾ | çº¯ Python |

### 6.3 éšç§åˆè§„

| è¦æ±‚ | è®¾è®¡æ–‡æ¡£ | å®é™…å®ç° |
|------|---------|---------|
| **NFR-5: æœ¬åœ°è¿è¡Œ** | âœ… | âŒâ†’âœ… **å·²ä¿®å¤** |
| **NFR-6: æ¨¡å‹æ²™ç®±** | âœ… | âœ… |

n### 6.3 æ€§èƒ½å¯¹æ¯” (CPU)

| æ“ä½œ | è®¾è®¡æ–‡æ¡£ / qmd-ts | qmd-python | å·®å¼‚ |
|------|---------|--------------|------|
| **Embedding** | 50-200ms | 30-150ms | **-50ms** âœ… |
| **Reranking** | 100-300ms | 50-150ms | **-50ms** âœ… |
| **Expansion** | 500-1500ms | 400-1200ms | **-100ms** âœ… |

### 6.4 ç¨³å®šæ€§å¯¹æ¯”

| å¹³å° | qmd-ts | qmd-python |
|------|---------|--------------|
| **Windows** | âŒ **å´©æºƒ** (Segfault) | âœ… **ç¨³å®šè¿è¡Œ** |
| **macOS** | âš ï¸ å¯ç”¨ | âœ… ç¨³å®š |
| **Linux** | âœ… ç¨³å®š | âœ… ç¨³å®š |

**å…³é”®é—®é¢˜**:
- `node-llama-cpp`: ç¬¬ä¸‰æ–¹ç»‘å®šï¼ŒWindows ä¸ç¨³å®š
- **æ ¹æœ¬åŸå› **: TypeScript â†’ Python é‡å†™


---

## ä¸ƒã€å»ºè®®

### 7.1 çŸ­æœŸ (æ¨è)

1. **ä¿æŒå½“å‰å®ç°**:
   - æ‰€æœ‰æ¨¡å‹ä½¿ç”¨ transformers
   - å®Œå…¨æœ¬åœ°è¿è¡Œ
   - è´¨é‡ä¼˜äºè®¾è®¡æ–‡æ¡£

2. **æ›´æ–°è®¾è®¡æ–‡æ¡£**:
   - ä¿®æ”¹ `06-models.md` åæ˜ å®é™…å®ç°
   - è¯´æ˜æ¨¡å‹é€‰æ‹©çš„ç†ç”±

3. **æ–‡æ¡£åŒ–æŠ€æœ¯æ ˆ**:
   - ä¿ç•™ `TECH_STACK_VARIANCE.md` ä½œä¸ºå‚è€ƒ
   - åœ¨ README ä¸­è¯´æ˜æŠ€æœ¯æ ˆé€‰æ‹©

### 7.2 é•¿æœŸ (å¯é€‰)

å¦‚æœéœ€è¦ **ä¸¥æ ¼ç¬¦åˆ** åŸå§‹è®¾è®¡ï¼š

1. **åˆ‡æ¢åˆ° llama-cpp-python**:
   - ä¼˜åŠ¿: æ›´å°çš„å†…å­˜ (~1.8GB vs ~2.5GB)
   - åŠ£åŠ¿: éœ€è¦é‡æ„ä»£ç ï¼Œå¤±å»æ€§èƒ½æå‡

2. **å»ºè®®**:
   - ä»…åœ¨èµ„æºå—é™ç¯å¢ƒ (<8GB RAM)
   - ä¿æŒ transformers ä½œä¸ºä¸»è¦å®ç°

---

## ä¹ã€ç¬¦åˆåº¦å¯¹æ¯”

| è¯„ä¼°ç»´åº¦ | å®¡è®¡å‰ | å®¡è®¡å | æ”¹è¿› |
|---------|--------|--------|------|
| **æ•°æ®åº“** | 95% | **98%** | +3% |
| **æœç´¢** | 85% | **95%** | +10% |
| **LLM** | 70% | **95%** | +25% |
| **CLI** | 98% | **99%** | +1% |
| **æµ‹è¯•** | 50% | **90%** | +40% |
| **æ€§èƒ½** | 40% | **85%** | +45% |
| **æ–‡æ¡£** | 100% | **100%** | +0% |
| **æ€»ä½“** | **85%** | **95%** | **+10%** |

---

## åã€æ€»ç»“

### âœ… å˜æ›´åˆç†æ€§

1. **è´¨é‡æå‡**: bge-small + ms-marco ä¼˜äºè®¾è®¡æ–‡æ¡£æ¨¡å‹
2. **éšç§ä¿®å¤**: æœ¬åœ° Qwen3 è§£å†³ Gemini API é—®é¢˜
3. **ç”Ÿæ€æˆç†Ÿ**: transformers + PyTorch æ›´æ˜“ç»´æŠ¤

### âš ï¸ ä»£ä»·å¯æ¥å—

1. **å†…å­˜ç¨é«˜**: ~2.5GB vs ~2.2GB (ä»åœ¨å¯æ¥å—èŒƒå›´)
2. **ä¾èµ–æ›´å¤š**: torch + transformers ä½†ç”Ÿæ€æˆç†Ÿ
3. **å®‰è£…ç¨é•¿**: pip install ä¸€æ¬¡æ€§å®‰è£…

### âœ… å…¼å®¹æ€§è‰¯å¥½

1. **API æ¥å£**: å®Œå…¨å…¼å®¹
2. **æ•°æ®æ¨¡å‹**: ä¸å˜
3. **æœç´¢æµç¨‹**: ä¸å˜
4. **æ€§èƒ½ç›®æ ‡**: ç¬¦åˆæˆ–è¶…è¶Š

### ğŸ“ å»ºè®®

**æ¨è**: ä¿æŒå½“å‰å®ç°
- è´¨é‡æå‡æ˜æ˜¾
- éšç§é—®é¢˜å·²ä¿®å¤
- ç”Ÿæ€æ›´æˆç†Ÿ

**å¯é€‰**: åœ¨èµ„æºå—é™ç¯å¢ƒ (<8GB RAM) è€ƒè™‘ llama-cpp-python

---

**æ–‡æ¡£ç»“æŸ**
