# Llama-cpp-python Integration - DEPRECATED

**Status**: ‚ùå **ABANDONED** - Not viable for production use
**Date**: 2026-02-19
**Reason**: Model compatibility issues and version constraints

---

## üìã Summary

This directory contains the failed attempt to integrate `llama-cpp-python` as an alternative embedding engine for QMD-Python.

### What Was Attempted

1. **Embedding Model**: BGE Small English v1.5 (Q8_0 GGUF)
   - ‚úÖ **Status**: Works! Good performance (5-15ms latency)
   - ‚ö†Ô∏è **Issue**: Model compatibility limited to older architectures

2. **Reranker Model**: Qwen3-Reranker-0.6B (Q8_0 GGUF)
   - ‚ùå **Status**: **FAILED** - Incompatible with llama-cpp-python 0.3.16
   - üî¥ **Error**: `wrong number of tensors; expected 311, got 310`

---

## ‚ùå Why This Approach Was Abandoned

### 1. Model Compatibility Issues

| Model | Status | Issue |
|-------|--------|-------|
| BGE Small English v1.5 | ‚úÖ Works | BERT architecture (old, compatible) |
| Gemma-2B Embedding | ‚ùå Failed | `gemma-embedding` architecture not supported |
| Qwen3-Reranker-0.6B | ‚ùå Failed | Qwen3 architecture not supported |

**Root Cause**: llama-cpp-python 0.3.16 is too old to support newer model architectures.

### 2. Version Upgrade Problems

**Attempted Solution**: Upgrade llama-cpp-python to latest version

```bash
pip install --upgrade llama-cpp-python \
  --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121
```

**Result**: No newer version available. The project already has the latest CUDA build.

**Project's pre-built wheels**:
- `llama_cpp_python-0.3.16-cp310-cp310-win_amd64.whl` (current)
- `llama_cpp_python-0.3.4-cp310-cp310-win_amd64.whl` (older)

**Conclusion**: No viable upgrade path.

### 3. Architectural Limitations

** llama-cpp-python is designed for**:
- Text generation (LLaMA, Mistral, Qwen, etc.)
- NOT for specialized embedding/reranker models
- GGUF format primarily optimized for generation tasks

**Our requirements**:
- Embedding models (BGE, GTE, E5, etc.)
- Reranker models (Qwen3-Reranker, BGE-Reranker)
- Cross-encoder architectures

**Mismatch**: llama-cpp-python's architecture doesn't align with our needs.

---

## üìä Performance Results (For Reference)

### What DID Work: BGE Small English v1.5

**Configuration**:
- Model: `bge-small-en-v1.5.Q8_0.gguf` (35 MB)
- GPU: NVIDIA GTX 1660 Ti (6GB)
- Software: llama-cpp-python 0.3.16 + CUDA 12.1

**Performance**:
```
Model load time:  149 ms
Short text:       5.48 ms latency, 187 texts/sec
Medium text:      7.36 ms latency, 135 texts/sec
Long text:        14.44 ms latency, 70 texts/sec
Batch (5 docs):   8.71 ms/doc average
GPU memory:       22 MB
Model size:       35 MB (vs 130 MB PyTorch)
```

**Advantages**:
- ‚úÖ 3.5x smaller model (quantization)
- ‚úÖ 3x lower latency (GPU optimization)
- ‚úÖ 20x lower GPU memory usage
- ‚úÖ 15x faster load time

**Disadvantages**:
- ‚ùå Limited to old model architectures (BERT only)
- ‚ùå Q8 quantization may reduce embedding quality
- ‚ùå No support for newer SOTA models

---

## üóÇÔ∏è Directory Structure

```
docs/deprecated/llama-cpp-python/
‚îú‚îÄ‚îÄ README.md (this file)
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ test_llama_embedding.py       # Embedding benchmark
‚îÇ   ‚îú‚îÄ‚îÄ test_llama_reranker.py        # Reranker benchmark
‚îÇ   ‚îú‚îÄ‚îÄ test_model_loading.py         # Model loading tests
‚îÇ   ‚îú‚îÄ‚îÄ test_qwen_reranker_loading.py # Qwen3 compatibility test
‚îÇ   ‚îú‚îÄ‚îÄ test_bge_model.py             # BGE model test
‚îÇ   ‚îú‚îÄ‚îÄ demo_llama_embedding.py       # Usage demo
‚îÇ   ‚îú‚îÄ‚îÄ download_gguf_model.py        # Model downloader
‚îÇ   ‚îî‚îÄ‚îÄ download_gguf_simple.py       # Simplified downloader
‚îú‚îÄ‚îÄ wheels/
‚îÇ   ‚îú‚îÄ‚îÄ llama_cpp_python-0.3.16-cp310-cp310-win_amd64.whl
‚îÇ   ‚îî‚îÄ‚îÄ llama_cpp_python-0.3.4-cp310-cp310-win_amd64.whl
‚îú‚îÄ‚îÄ reports/
‚îÇ   ‚îú‚îÄ‚îÄ LLAMA_CPP_PERFORMANCE_REPORT.md   # Detailed analysis
‚îÇ   ‚îî‚îÄ‚îÄ embedding_benchmark_results.json  # Raw benchmark data
‚îî‚îÄ‚îÄ models/
    ‚îî‚îÄ‚îÄ (downloaded GGUF models - if any)
```

---

## üí° Lessons Learned

### 1. Not All "Optimized" Solutions Are Better

**Expectation**: llama-cpp-python would be faster and more efficient
**Reality**: Only works for old architectures (BERT), not modern SOTA models

### 2. Check Model Compatibility FIRST

**Mistake**: Assumed all GGUF models would work
**Reality**: Different architectures have different compatibility requirements

### 3. Version Matters

**Problem**: llama-cpp-python 0.3.16 lacks support for newer models
**Solution**: No easy upgrade path (would require compiling from source)

### 4. PyTorch Is Still the Best Choice for Embeddings

**Why**:
- ‚úÖ Supports all model architectures
- ‚úÖ Active development (transformers, sentence-transformers)
- ‚úÖ Better model quality (FP16/BF16 vs Q8 quantization)
- ‚úÖ Ecosystem integration (ChromaDB, Qdrant, etc.)

---

## üîÑ Current Solution (PyTorch)

QMD-Python continues to use **PyTorch-based embedding**:

```python
from fastembed import TextEmbedding

model = TextEmbedding(
    model_name="BAAI/bge-small-en-v1.5",
    providers=["CUDAExecutionProvider"]  # GPU acceleration
)
embeddings = list(model.embed(texts))
```

**Performance**:
- Latency: ~15-20 ms (acceptable)
- Throughput: ~50-70 texts/sec
- GPU Memory: ~500 MB
- Model Size: 130 MB

**Trade-offs**:
- ‚ùå Higher latency than llama-cpp-python (but still acceptable)
- ‚ùå Higher GPU memory usage
- ‚úÖ **Better model quality** (FP16 vs Q8)
- ‚úÖ **Supports all modern architectures**
- ‚úÖ **Active maintenance and updates**

---

## üöÄ Future Considerations

### If llama-cpp-python Adds Support

**Monitor**:
- llama-cpp-python GitHub repository
- GGUF model support for newer architectures
- Cross-encoder/reranker model support

**Re-evaluation Criteria**:
1. ‚úÖ Support for BGE-Reranker models
2. ‚úÖ Support for Qwen3-Reranker
3. ‚úÖ Performance >2x improvement over PyTorch
4. ‚úÖ Model quality parity (no quantization loss)

### Alternative Approaches

1. **ONNX Runtime** (already used by fastembed)
   - ‚úÖ Good performance
   - ‚úÖ Wide model support
   - ‚úÖ Already integrated

2. **TensorRT** (NVIDIA-only)
   - ‚ö†Ô∏è Complex setup
   - ‚ö†Ô∏è NVIDIA GPU required
   - ‚úÖ Maximum performance

3. **MLC LLM**
   - ‚ö†Ô∏è Newer, less mature
   - ‚úÖ Promising performance

---

## üìù Conclusion

**llama-cpp-python is NOT suitable for QMD-Python's needs** because:
1. ‚ùå Incompatible with modern embedding/reranker models
2. ‚ùå No viable upgrade path
3. ‚ùå Architectural mismatch with our requirements

**Decision**: Stick with **PyTorch + fastembed** for production use.

**Benefit**: This exploration confirmed that PyTorch remains the best choice, despite higher resource usage.

---

**Archived by**: AI Agent (Sisyphus)
**Archive Date**: 2026-02-19
**Reason**: Model compatibility issues make this approach non-viable
