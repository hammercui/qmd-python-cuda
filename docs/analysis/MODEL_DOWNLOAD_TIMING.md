# æ¨¡å‹ä¸‹è½½æ—¶æœºè¯´æ˜

> **ç‰ˆæœ¬**: qmd-python 0.1.0 (BGE-M3)
> **æ›´æ–°æ—¥æœŸ**: 2026-02-19

---

## ğŸ“¥ ä»€ä¹ˆæ—¶å€™ä¼šä¸‹è½½æ–°æ¨¡å‹ï¼Ÿ

### å¿«é€Ÿç­”æ¡ˆ

**é¦–æ¬¡ä½¿ç”¨éœ€è¦æ¨¡å‹çš„åŠŸèƒ½æ—¶**ï¼Œä¼šè‡ªåŠ¨ä» HuggingFace æˆ– ModelScope ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ç¼“å­˜ã€‚

---

## ğŸ”„ æ¨¡å‹ä¸‹è½½è§¦å‘ç‚¹

### 1. CLI Embed å‘½ä»¤

**å‘½ä»¤**: `qmd embed`

**è§¦å‘æ—¶æœº**:
```python
# qmd/cli.py, line 649-654
@cli.command()
@click.option("--mode", default="auto")
def embed(ctx_obj, collection, force, mode):
    from qmd.llm.engine import LLMEngine
    llm = LLMEngine(mode=mode)  # â† é¦–æ¬¡è°ƒç”¨æ—¶ä¸‹è½½æ¨¡å‹
    ...
```

**ä¸‹è½½å†…å®¹**:
- **BGE-M3 INT8**: 542 MB (æ¥è‡ª `Xenova/bge-m3`)
- **ç¼“å­˜ä½ç½®**: `~/.cache/huggingface/hub/models--Xenova--bge-m3/`

**é¦–æ¬¡ embed æ—¶é—´**:
- CPU: ~5-10 åˆ†é’Ÿ (ä¸‹è½½ + åˆå§‹åŒ–)
- GPU: ~2-5 åˆ†é’Ÿ (ä¸‹è½½ + åˆå§‹åŒ–)

**åç»­ embed æ—¶é—´**:
- CPU: ~15-30ms/chunk (å·²ç¼“å­˜)
- GPU: ~5-10ms/chunk (å·²ç¼“å­˜)

---

### 2. CLI Server å¯åŠ¨

**å‘½ä»¤**: `qmd server`

**è§¦å‘æ—¶æœº**:
```python
# qmd/server/app.py, line 63-96
@app.on_event("startup")
async def startup_event():
    ...
    from fastembed import TextEmbedding
    _model = TextEmbedding(model_name=DEFAULT_MODEL)  # â† å¯åŠ¨æ—¶ä¸‹è½½æ¨¡å‹
    ...
```

**ä¸‹è½½å†…å®¹**: åŒä¸Š (BGE-M3 INT8, 542 MB)

**å¯åŠ¨æ—¶é—´**:
- é¦–æ¬¡: ~5-10 åˆ†é’Ÿ (ä¸‹è½½ + åŠ è½½)
- åç»­: ~10-30 ç§’ (ä»…åŠ è½½ï¼Œå·²ç¼“å­˜)

---

### 3. CLI VSearch / Query å‘½ä»¤

**å‘½ä»¤**: `qmd vsearch` æˆ– `qmd query`

**è§¦å‘æ—¶æœº**:
```python
# qmd/cli.py, line 794-831
@cli.command()
def vsearch(ctx_obj, query, collection, limit):
    from qmd.server.client import EmbedServerClient
    client = EmbedServerClient()
    results = client.vsearch(query, ...)  # â† å¦‚æœ Server æœªè¿è¡Œï¼Œè‡ªåŠ¨å¯åŠ¨ Server
    ...
```

**æ³¨æ„**: è¿™äº›å‘½ä»¤**ä¸ä¼šç›´æ¥ä¸‹è½½æ¨¡å‹**ï¼Œè€Œæ˜¯ï¼š
1. å°è¯•è¿æ¥å·²è¿è¡Œçš„ Server
2. å¦‚æœ Server æœªè¿è¡Œï¼Œè‡ªåŠ¨å¯åŠ¨ Server (æ­¤æ—¶ä¸‹è½½æ¨¡å‹)
3. é€šè¿‡ HTTP API è°ƒç”¨ Server

---

### 4. æ‰‹åŠ¨ä¸‹è½½å‘½ä»¤

**å‘½ä»¤**: `qmd check --download` æˆ– `python -m qmd.models.downloader`

**è§¦å‘æ—¶æœº**: ç”¨æˆ·ä¸»åŠ¨æ‰§è¡Œ

**ä¸‹è½½å†…å®¹**:
- **BGE-M3 INT8**: 542 MB (embed æ¨¡å‹)
- **Qwen3-Reranker**: ~400 MB (é‡æ’åºæ¨¡å‹)
- **Qwen3-0.6B-Instruct**: ~400 MB (æŸ¥è¯¢æ‰©å±•æ¨¡å‹)
- **æ€»è®¡**: ~1.3 GB

**ç¼“å­˜ä½ç½®**: `~/.cache/qmd/models/`

---

## ğŸ“ æ¨¡å‹ç¼“å­˜ä½ç½®

### FastEmbed ç¼“å­˜ (BGE-M3)

**é»˜è®¤ä½ç½®**:
```
~/.cache/huggingface/hub/models--Xenova--bge-m3/
  â”œâ”€â”€ onnx/
  â”‚   â”œâ”€â”€ model_int8.onnx        # 542 MB (ä¸»æ–‡ä»¶)
  â”‚   â”œâ”€â”€ tokenizer.json
  â”‚   â”œâ”€â”€ tokenizer_config.json
  â”‚   â””â”€â”€ sentencepiece.bpe.model
  â””â”€â”€ ...
```

**è‡ªå®šä¹‰ç¼“å­˜ä½ç½®**:
```python
llm = LLMEngine(cache_dir="/custom/path")
```

### ModelDownloader ç¼“å­˜ (æ‰‹åŠ¨ä¸‹è½½çš„æ¨¡å‹)

**é»˜è®¤ä½ç½®**:
```
~/.cache/qmd/models/
  â”œâ”€â”€ onnx-int8_embedding/       # BGE-M3 (å¦‚æœæ‰‹åŠ¨ä¸‹è½½)
  â”œâ”€â”€ onnx-reranker_reranker/    # Qwen3-Reranker
  â””â”€â”€ onnx-causal-lm_expansion/  # Qwen3-0.6B-Instruct
```

---

## ğŸ” æ¨¡å‹ä¸‹è½½æ£€æµ‹æœºåˆ¶

### FastEmbed è‡ªåŠ¨æ£€æµ‹

**é€»è¾‘**:
```python
# qmd/llm/engine.py, line 184-228
def _ensure_model(self) -> None:
    """Load model if not already loaded (standalone mode)."""
    if self._model is not None:
        return  # å·²åŠ è½½ï¼Œç›´æ¥è¿”å›

    # æ³¨å†Œ BGE-M3 è‡ªå®šä¹‰æ¨¡å‹
    if self.model_name == "BAAI/bge-m3":
        _register_bge_m3()  # åªæ³¨å†Œä¸€æ¬¡

    # å°è¯•ä½¿ç”¨æœ¬åœ°ç¼“å­˜
    if self._downloader is None:
        self._downloader = ModelDownloader()

    cached_path = self._downloader.get_model_path("embedding")
    if cached_path:
        model_path = str(cached_path)  # ä½¿ç”¨ç¼“å­˜
    else:
        model_path = self.model_name  # è§¦å‘ä¸‹è½½

    # åŠ è½½æ¨¡å‹ (å¦‚æœæœ¬åœ°æ²¡æœ‰ï¼Œfastembed ä¼šè‡ªåŠ¨ä¸‹è½½)
    from fastembed import TextEmbedding
    self._model = TextEmbedding(
        model_name=model_path,  # â† è¿™é‡Œè§¦å‘ä¸‹è½½
        cache_dir=self.cache_dir,
        providers=providers,
    )
```

**æ£€æµ‹ä¼˜å…ˆçº§**:
1. **å·²åŠ è½½**: `_model is not None` â†’ ç›´æ¥è¿”å›
2. **æœ¬åœ°ç¼“å­˜**: `~/.cache/qmd/models/` â†’ ä½¿ç”¨ç¼“å­˜
3. **HuggingFace ç¼“å­˜**: `~/.cache/huggingface/hub/` â†’ ä½¿ç”¨ç¼“å­˜
4. **è‡ªåŠ¨ä¸‹è½½**: ä» `Xenova/bge-m3` ä¸‹è½½

---

## ğŸš€ ä¸‹è½½æµç¨‹

### å®Œæ•´ä¸‹è½½æµç¨‹

```
ç”¨æˆ·æ‰§è¡Œ: qmd embed
    â†“
CLI è°ƒç”¨: LLMEngine(mode="auto")
    â†“
LLMEngine._ensure_model()
    â†“
æ³¨å†Œ BGE-M3: _register_bge_m3()
    â”œâ”€ TextEmbedding.add_custom_model(...)
    â””â”€ è®¾ç½® sources=ModelSource(hf="Xenova/bge-m3")
    â†“
æ£€æŸ¥ç¼“å­˜: ModelDownloader().get_model_path("embedding")
    â”œâ”€ æ‰¾åˆ°ç¼“å­˜ â†’ ä½¿ç”¨æœ¬åœ°è·¯å¾„
    â””â”€ æœªæ‰¾åˆ° â†’ ç»§ç»­
    â†“
FastEmbed åŠ è½½: TextEmbedding(model="BAAI/bge-m3")
    â”œâ”€ æ£€æŸ¥ ~/.cache/huggingface/hub/models--Xenova--bge-m3/
    â”œâ”€ å­˜åœ¨ â†’ ç›´æ¥åŠ è½½
    â””â”€ ä¸å­˜åœ¨ â†’ è§¦å‘ä¸‹è½½
        â†“
    ä» HuggingFace ä¸‹è½½:
    â””â”€ https://huggingface.co/Xenova/bge-m3
        â”œâ”€ onnx/model_int8.onnx (542 MB)
        â”œâ”€ onnx/tokenizer.json
        â”œâ”€ onnx/tokenizer_config.json
        â””â”€ onnx/sentencepiece.bpe.model
        â†“
    ä¿å­˜åˆ°: ~/.cache/huggingface/hub/models--Xenova--bge-m3/
        â†“
    åŠ è½½åˆ°å†…å­˜ (ONNX Runtime)
        â†“
    å¼€å§‹ embed å¤„ç†
```

### ä¸‹è½½æ—¶é—´ä¼°ç®—

**ç½‘ç»œæ¡ä»¶**: 100 Mbps å®¶ç”¨å®½å¸¦

| æ¨¡å‹ | å¤§å° | ä¸‹è½½æ—¶é—´ | åˆå§‹åŒ–æ—¶é—´ | æ€»è®¡ |
|------|------|----------|-----------|------|
| **BGE-M3 INT8** | 542 MB | ~45 ç§’ | ~30 ç§’ | **~75 ç§’** |
| Qwen3-Reranker | 400 MB | ~30 ç§’ | ~20 ç§’ | ~50 ç§’ |
| Qwen3-0.6B-Instruct | 400 MB | ~30 ç§’ | ~20 ç§’ | ~50 ç§’ |
| **æ€»è®¡** | 1.3 GB | ~105 ç§’ | ~70 ç§’ | **~175 ç§’** |

**åŠ é€Ÿæ–¹æ¡ˆ**:
- âœ… å›½å†…ç”¨æˆ· â†’ è‡ªåŠ¨ä½¿ç”¨ ModelScope (é€Ÿåº¦æå‡ 5-10x)
- âœ… æå‰ä¸‹è½½ â†’ `qmd check --download` (åå°ä¸‹è½½)

---

## ğŸ› ï¸ æ‰‹åŠ¨ä¸‹è½½å‘½ä»¤

### æ–¹æ³• 1: qmd check (æ¨è)

```bash
# æ£€æµ‹ç³»ç»ŸçŠ¶æ€å¹¶è‡ªåŠ¨ä¸‹è½½ç¼ºå¤±çš„æ¨¡å‹
qmd check --download

# è¾“å‡ºç¤ºä¾‹:
# [bold yellow]Starting model download...[/bold yellow]
# [dim]Cache directory: C:\Users\YourName\.cache\qmd\models[/dim]
# [cyan][ModelScope] Downloading embedding...[/cyan]
# â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100%
# [green][MoT] OK Downloaded to ...\onnx-int8_embedding[/green]
# [bold green]Download complete![/bold green]
```

### æ–¹æ³• 2: Python æ¨¡å—

```bash
# ç›´æ¥è¿è¡Œä¸‹è½½å™¨
python -m qmd.models.downloader

# æ•ˆæœåŒä¸Šï¼Œæ”¯æŒåŒæºå¹¶è¡Œä¸‹è½½
```

### æ–¹æ³• 3: é¢„ä¸‹è½½ (ç¦»çº¿åœºæ™¯)

```bash
# 1. åœ¨æœ‰ç½‘ç»œçš„æœºå™¨ä¸Šä¸‹è½½
python -m qmd.models.downloader

# 2. å¤åˆ¶ç¼“å­˜ç›®å½•åˆ°ç¦»çº¿æœºå™¨
# æº: ~/.cache/huggingface/hub/
# ç›®æ ‡: ~/.cache/huggingface/hub/

# 3. åœ¨ç¦»çº¿æœºå™¨ä¸Šç›´æ¥ä½¿ç”¨
qmd embed  # æ— éœ€ä¸‹è½½ï¼Œç›´æ¥ä½¿ç”¨ç¼“å­˜
```

---

## ğŸŒ ä¸‹è½½æºé€‰æ‹©

### è‡ªåŠ¨æ£€æµ‹ (é»˜è®¤)

**é€»è¾‘**:
```python
# qmd/models/downloader.py, line 28-66
def _detect_location() -> str:
    """Detect if running in China or Overseas."""
    # 1. æ£€æŸ¥æ—¶åŒº
    if "Asia/Shanghai" in time.tzname:
        return "cn"  # ä¸­å›½ â†’ ModelScope

    # 2. å›é€€åˆ° IP æ£€æµ‹
    response = requests.get("http://ip-api.com/json/")
    if response.json()["country_code"] == "CN":
        return "cn"  # ä¸­å›½ IP â†’ ModelScope

    return "global"  # å…¶ä»– â†’ HuggingFace
```

**è§„åˆ™**:
- ğŸ‡¨ğŸ‡³ **ä¸­å›½ç”¨æˆ·** â†’ è‡ªåŠ¨ä½¿ç”¨ **ModelScope** (é­”æ­ç¤¾åŒº)
  - é€Ÿåº¦å¿« (å›½å†…æœåŠ¡å™¨)
  - æ— éœ€ç¿»å¢™
- ğŸŒ **æµ·å¤–ç”¨æˆ·** â†’ è‡ªåŠ¨ä½¿ç”¨ **HuggingFace**
  - é€Ÿåº¦å¿« (å…¨çƒ CDN)
  - æ¨¡å‹æ›´æ–°å¿«

### æ‰‹åŠ¨æŒ‡å®šä¸‹è½½æº

**ç¼–è¾‘é…ç½®æ–‡ä»¶**:
```yaml
# ~/.qmd/index.yml
model_source: "modelscope"  # å¼ºåˆ¶ä½¿ç”¨ ModelScope
# æˆ–
model_source: "huggingface"  # å¼ºåˆ¶ä½¿ç”¨ HuggingFace
```

**æˆ–ç¯å¢ƒå˜é‡**:
```bash
export QMD_MODEL_SOURCE=huggingface  # Linux/macOS
set QMD_MODEL_SOURCE=huggingface     # Windows
```

---

## ğŸ” æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½

### æ–¹æ³• 1: qmd status

```bash
$ qmd status

Index size: 5.2 MB
Collections: 1
Documents: 199
Embeddings: 199/199 (100.0%)
```

**æ³¨æ„**: `qmd status` **ä¸ä¼š**æ˜¾ç¤ºæ¨¡å‹æ˜¯å¦å·²ä¸‹è½½ï¼Œåªæ˜¾ç¤ºæ•°æ®åº“çŠ¶æ€ã€‚

### æ–¹æ³• 2: qmd check

```bash
$ qmd check

System Status Check
====================

Dependencies:
  âœ“ torch v2.1.0
  âœ“ transformers v4.30.0
  âœ“ fastembed Installed

Device:
  OK CUDA: NVIDIA GeForce RTX 3060
  GPU Count: 1
  CUDA Version: 12.1
  GPU 0: RTX 3060 (12.0 GB, Compute 8.6)

Models:
  OK embedding (600MB)  â† å·²ä¸‹è½½
  X reranker (400MB)     â† æœªä¸‹è½½
  X expansion (400MB)    â† æœªä¸‹è½½

Recommendations:
  [yellow]Run:[/yellow] qmd download  # Download all models
```

### æ–¹æ³• 3: æ£€æŸ¥ç¼“å­˜ç›®å½•

```bash
# Linux/macOS
ls -lh ~/.cache/huggingface/hub/ | grep bge-m3

# Windows
dir %USERPROFILE%\.cache\huggingface\hub\ | findstr bge-m3
```

**é¢„æœŸè¾“å‡º**:
```
drwxr-xr-x  10 user  staff   320B Feb 19 10:30 models--Xenova--bge-m3
```

---

## âš¡ æœ€ä½³å®è·µ

### æ¨èå·¥ä½œæµç¨‹

**åœºæ™¯ 1: é¦–æ¬¡å®‰è£…**
```bash
# 1. å®‰è£…ä¾èµ–
pip install -e .[cuda]

# 2. é¢„ä¸‹è½½æ‰€æœ‰æ¨¡å‹ (åå°ï¼Œé¿å…é¦–æ¬¡ä½¿ç”¨æ—¶ç­‰å¾…)
qmd check --download

# 3. ç´¢å¼•æ–‡æ¡£
qmd index

# 4. ç”Ÿæˆå‘é‡
qmd embed  # æ¨¡å‹å·²ç¼“å­˜ï¼Œç›´æ¥ä½¿ç”¨
```

**åœºæ™¯ 2: æ—¥å¸¸ä½¿ç”¨**
```bash
# 1. æ›´æ–°æ–‡æ¡£
git pull  # æˆ–ä¿®æ”¹ markdown æ–‡ä»¶

# 2. é‡æ–°ç´¢å¼•
qmd index  # å¿«é€Ÿ (ä»…æ‰«ææ–‡ä»¶ç³»ç»Ÿ)

# 3. æœç´¢
qmd query "your query"  # å³æ—¶è¿”å›
```

**åœºæ™¯ 3: ç¦»çº¿ç¯å¢ƒ**
```bash
# 1. åœ¨çº¿æœºå™¨ä¸Šé¢„ä¸‹è½½
qmd check --download

# 2. å¤åˆ¶ç¼“å­˜åˆ°ç¦»çº¿æœºå™¨
# æº: ~/.cache/huggingface/hub/
# ç›®æ ‡: ~/.cache/huggingface/hub/

# 3. ç¦»çº¿ä½¿ç”¨ (æ— éœ€ç½‘ç»œ)
qmd embed
qmd query "offline query"
```

---

## ğŸ› å¸¸è§é—®é¢˜

### Q1: ä¸‹è½½é€Ÿåº¦æ…¢

**åŸå› **: é»˜è®¤ä» HuggingFace ä¸‹è½½ï¼Œæµ·å¤–æœåŠ¡å™¨

**è§£å†³**:
```bash
# æ–¹æ¡ˆ 1: ä½¿ç”¨å›½å†…é•œåƒ (è‡ªåŠ¨)
# ç¡®ä¿åœ¨æ—¶åŒº "Asia/Shanghai" æˆ– IP åœ¨ä¸­å›½
# QMD ä¼šè‡ªåŠ¨ä½¿ç”¨ ModelScope

# æ–¹æ¡ˆ 2: æ‰‹åŠ¨æŒ‡å®š ModelScope
# ç¼–è¾‘ ~/.qmd/index.yml
model_source: "modelscope"

# æ–¹æ¡ˆ 3: æå‰ä¸‹è½½
qmd check --download  # åå°ä¸‹è½½ï¼Œé¿å…é¦–æ¬¡ä½¿ç”¨æ—¶ç­‰å¾…
```

### Q2: ä¸‹è½½å¤±è´¥ (ç½‘ç»œé”™è¯¯)

**è§£å†³**:
```bash
# æ–¹æ¡ˆ 1: é‡è¯• (ç½‘ç»œæ³¢åŠ¨)
qmd check --download

# æ–¹æ¡ˆ 2: æ‰‹åŠ¨ä¸‹è½½
# 1. è®¿é—® https://huggingface.co/Xenova/bge-m3
# 2. ä¸‹è½½ onnx/model_int8.onnx (542 MB)
# 3. æ‰‹åŠ¨æ”¾ç½®åˆ° ~/.cache/huggingface/hub/models--Xenova--bge-m3/onnx/

# æ–¹æ¡ˆ 3: ä½¿ç”¨ä»£ç†
export HTTP_PROXY=http://127.0.0.1:7890
export HTTPS_PROXY=http://127.0.0.1:7890
qmd check --download
```

### Q3: ç£ç›˜ç©ºé—´ä¸è¶³

**éœ€æ±‚**:
- BGE-M3: 542 MB
- Reranker: 400 MB (å¯é€‰)
- Expansion: 400 MB (å¯é€‰)
- **æ€»è®¡**: ~1.3 GB

**è§£å†³**:
```bash
# æ–¹æ¡ˆ 1: æ¸…ç†ç¼“å­˜
rm -rf ~/.cache/huggingface/hub/  # Linux/macOS
del %USERPROFILE%\.cache\huggingface\hub\  # Windows

# æ–¹æ¡ˆ 2: è‡ªå®šä¹‰ç¼“å­˜ä½ç½®
# åœ¨ä»£ç ä¸­æŒ‡å®š:
llm = LLMEngine(cache_dir="/path/to/larger/disk")
```

### Q4: æ¨¡å‹æŸå (åŠ è½½å¤±è´¥)

**ç—‡çŠ¶**: `RuntimeError: Failed to load model`

**è§£å†³**:
```bash
# åˆ é™¤æŸåçš„ç¼“å­˜ï¼Œé‡æ–°ä¸‹è½½
rm -rf ~/.cache/huggingface/hub/models--Xenova--bge-m3/
qmd check --download
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- **å‡çº§å®Œæˆæ–‡æ¡£**: [UPGRADE_COMPLETE.md](UPGRADE_COMPLETE.md)
- **å¿«é€Ÿè¿ç§»æŒ‡å—**: [MIGRATION_GUIDE.md](MIGRATION_GUIDE.md)
- **æ¨¡å‹é…ç½®æ–‡æ¡£**: [../guide/FINAL_CONFIG.md](../guide/FINAL_CONFIG.md)

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0
**æ›´æ–°æ—¥æœŸ**: 2026-02-19
**ä½œè€…**: AI Assistant (OpenCode + GLM-4.7)
