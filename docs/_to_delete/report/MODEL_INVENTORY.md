# QMD-Python æ¨¡å‹ä½¿ç”¨æ¸…å•

**æ–‡æ¡£ç‰ˆæœ¬**: 1.1
**æ—¥æœŸ**: 2026-02-14
**çŠ¶æ€**: å½“å‰å®ç°

---

## æ‰§è¡Œæ‘˜è¦

æœ¬æ–‡æ¡£è¯¦ç»†åˆ—å‡º QMD-Python ç³»ç»Ÿä½¿ç”¨çš„**æ‰€æœ‰æ¨¡å‹**åŠå…¶**åº”ç”¨ç¯èŠ‚**ï¼ŒåŒ…æ‹¬**CPU/GPU è®¡ç®—è®¾å¤‡**ã€‚

**æ¨¡å‹æ€»è§ˆ**:
| # | æ¨¡å‹åç§° | ç±»å‹ | å¤§å° | ç”¨é€” | é»˜è®¤è®¾å¤‡ | GPU æ”¯æŒ |
|---|---------|------|------|------|---------|---------|
| **1** | `bge-small-en-v1.5` | Embedding | 130MB | å‘é‡åµŒå…¥ | **CPU** | âœ… å¯é€‰ |
| **2** | `ms-marco-MiniLM-L-6-v2` | Reranking | 110MB | ç»“æœé‡æ’ | **CPU** | âœ… å¯é€‰ |
| **3** | `Qwen3-0.5B-Instruct` | Expansion | 1.0GB | æŸ¥è¯¢æ‰©å±• | **CPU** | âœ… å¯é€‰ |
| **æ€»è®¡** | - | - | **~1.24GB** | **CPU** | - |

**å…³é”®å‘ç°**:
- âœ… **3 ä¸ªæ¨¡å‹**ï¼Œå…¨éƒ¨æ¥è‡ª HuggingFace
- âœ… **è‡ªåŠ¨ä¸‹è½½**ï¼Œé¦–æ¬¡ä½¿ç”¨æ—¶ä» HF Hub è·å–
- âœ… **å®Œå…¨æœ¬åœ°**ï¼Œæ— ç½‘ç»œè°ƒç”¨
- âœ… **ç¼“å­˜æœºåˆ¶**ï¼Œé¿å…é‡å¤åŠ è½½
- ğŸ”§ **é»˜è®¤ CPU**ï¼ŒGPU åŠ é€Ÿå¯é€‰ (éœ€ CUDA)

---

## ä¸€ã€æ¨¡å‹è¯¦ç»†æ¸…å•

### 1. Embedding æ¨¡å‹ï¼š`bge-small-en-v1.5`

#### åŸºæœ¬ä¿¡æ¯

| å±æ€§ | å€¼ |
|------|------|
| **å®Œæ•´åç§°** | `BAAI/bge-small-en-v1.5` |
| **åŸºç¡€æ¨¡å‹** | bge-small (åŒ—äº¬æ™ºæº) |
| **ç±»å‹** | Sentence Transformer (Embedding) |
| **æ–‡ä»¶å¤§å°** | ~130MB |
| **åµŒå…¥ç»´åº¦** | 384 |
| **ä¸Šä¸‹æ–‡çª—å£** | 512 tokens |
| **è¾“å‡º** | å‘é‡ (list[float], 384-dim) |
| **æ ¼å¼** | PyTorch (FP16) |
| **é»˜è®¤è®¾å¤‡** | **CPU** (fastembed) |
| **GPU æ”¯æŒ** | âœ… æ˜¯ (éœ€ CUDA) |
| **HuggingFace** | https://huggingface.co/BAAI/bge-small-en-v1.5 |

#### æ€§èƒ½ç‰¹ç‚¹

| æŒ‡æ ‡ | CPU | GPU (CUDA) |
|------|-----|-----------|
| **MTEB æ’å** | ä¼˜å¼‚ (SOTA) | ä¼˜å¼‚ (SOTA) |
| **æ£€ç´¢ä»»åŠ¡** | 62.5 (Retrieval) | 62.5 |
| **è¯­ä¹‰æœç´¢** | 81.2 (STS) | 81.2 |
| **é¦–æ¬¡æŸ¥è¯¢** | 500-2000ms | 200-500ms |
| **åç»­æŸ¥è¯¢** | 30-150ms | **10-50ms** (3x) |
| **æ‰¹é‡å¤„ç†** | ~100 chunks/s | ~300 chunks/s |

#### ä½¿ç”¨ç¯èŠ‚

#### 1.1 æ–‡æ¡£åµŒå…¥ (Indexing)

**å‘½ä»¤**: `qmd embed`

**è®¡ç®—è®¾å¤‡**: **CPU** (é»˜è®¤) æˆ– **GPU** (å¯é€‰)

**ä»£ç ä½ç½®**: `qmd/cli.py` â†’ `embed()` â†’ `qmd/llm/engine.py`

**æµç¨‹**:
```python
# qmd/cli.py (lines 511-527)
def embed(ctx_obj, collection):
    # è·å–éœ€è¦åµŒå…¥çš„æ–‡æ¡£
    to_embed = [doc for doc in col_docs if doc["embedding"] is None]
    
    # ç”ŸæˆåµŒå…¥ (CPU æˆ– GPU)
    from qmd.llm.engine import LLMEngine
    llm = LLMEngine()
    new_embeddings = llm.embed_texts(contents)  # è®¾å¤‡è‡ªåŠ¨é€‰æ‹©
    
    # ç¼“å­˜åˆ° SQLite (BLOB)
    doc["embedding"] = new_embeddings[i]
    ctx_obj.db.update_content_embedding(doc["hash"], np.array(new_embeddings[i]))
    
    # æ·»åŠ åˆ° ChromaDB (CPUï¼Œå¯é€‰ GPU)
    vsearch.add_documents_with_embeddings(col_name, col_docs)
```

**è°ƒç”¨**:
```python
# qmd/llm/engine.py (lines 32-39)
def embed_texts(self, texts: List[str]) -> List[List[float]]:
    """Generate embeddings for a list of texts."""
    self._ensure_model()  # åŠ è½½ bge-small (CPU/GPU)
    assert self._model is not None
    
    # fastembed è¿”å› numpy æ•°ç»„è¿­ä»£å™¨
    # è®¾å¤‡ï¼šCPU (é»˜è®¤) æˆ– GPU (è‡ªåŠ¨æ£€æµ‹)
    embeddings = list(self._model.embed(texts))
    return [emb.tolist() for emb in embeddings]
```

**æ€§èƒ½**:
- **CPU**: å•æ–‡æ¡£ 30-50msï¼Œæ‰¹é‡ (100) ~1s
- **GPU**: å•æ–‡æ¡£ **10-20ms** (3x)ï¼Œæ‰¹é‡ (100) **~300ms** (3x)
- ç¼“å­˜åˆ° SQLite (BLOB)
- å­˜å‚¨åˆ° ChromaDB (HNSW ç´¢å¼•ï¼ŒCPU)

#### 1.2 æŸ¥è¯¢åµŒå…¥ (Vector Search)

**å‘½ä»¤**: `qmd vsearch "query"`

**è®¡ç®—è®¾å¤‡**: **CPU** (é»˜è®¤) æˆ– **GPU** (å¯é€‰)

**ä»£ç ä½ç½®**: `qmd/search/vector.py` â†’ `search()`

**æµç¨‹**:
```python
# qmd/search/vector.py (lines 67-76)
def search(self, query: str, collection_name: str, limit: int = 5):
    """Perform semantic search."""
    collection = self._get_collection(collection_name)
    
    # ç”ŸæˆæŸ¥è¯¢åµŒå…¥ (CPU æˆ– GPU)
    query_embedding = self.llm.embed_query(query)  # 10-50ms
    
    # ChromaDB å‘é‡æœç´¢ (HNSWï¼ŒCPU)
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=limit,
        include=["documents", "metadatas", "distances"]
    )  # 50-200ms
```

**è°ƒç”¨**:
```python
# qmd/llm/engine.py (lines 41-43)
def embed_query(self, text: str) -> List[float]:
    """Generate embedding for a single query."""
    return self.embed_texts([text])[0]  # CPU æˆ– GPU
```

**æ€§èƒ½**:
- **CPU**: å•æŸ¥è¯¢ 30-50msï¼ŒChromaDB æœç´¢ 50-200msï¼Œæ€»è®¡ **80-250ms**
- **GPU**: å•æŸ¥è¯¢ **10-20ms**ï¼ŒChromaDB æœç´¢ 50-200msï¼Œæ€»è®¡ **60-220ms**

#### 1.3 æ··æˆæœç´¢ (Hybrid Search)

**å‘½ä»¤**: `qmd query "query"`

**è®¡ç®—è®¾å¤‡**: **CPU** (é»˜è®¤) æˆ– **GPU** (å¯é€‰)

**ä»£ç ä½ç½®**: `qmd/search/hybrid.py` â†’ `search()`

**æµç¨‹**:
```python
# qmd/search/hybrid.py (lines 28-35)
# 2. Get Vector results
# Vector score: higher is better. Results are already sorted.
vector_results = self.vector.search(query, collection or "default", limit=limit * 2)
```

**æ€§èƒ½**:
- **CPU**: åŒå‘é‡æœç´¢ 80-250ms
- **GPU**: åŒå‘é‡æœç´¢ 60-220ms
- RRF èåˆ: 10-20ms

---

### 2. Reranking æ¨¡å‹ï¼š`ms-marco-MiniLM-L-6-v2`

#### åŸºæœ¬ä¿¡æ¯

| å±æ€§ | å€¼ |
|------|------|
| **å®Œæ•´åç§°** | `cross-encoder/ms-marco-MiniLM-L-6-v2` |
| **åŸºç¡€æ¨¡å‹** | MiniLM-L-6 (Microsoft) |
| **ç±»å‹** | Cross-Encoder (Reranking) |
| **æ–‡ä»¶å¤§å°** | ~110MB |
| **è¾“å‡º** | ç›¸å…³æ€§åˆ†æ•° (0.0 - 1.0) |
| **ä¸Šä¸‹æ–‡çª—å£** | 512 tokens |
| **æ ¼å¼** | PyTorch (FP32) |
| **é»˜è®¤è®¾å¤‡** | **CPU** (transformers) |
| **GPU æ”¯æŒ** | âœ… æ˜¯ (éœ€ CUDA) |
| **HuggingFace** | https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-6-v2 |

#### æ€§èƒ½ç‰¹ç‚¹

| æŒ‡æ ‡ | CPU | GPU (CUDA) |
|------|-----|-----------|
| **è®­ç»ƒä»»åŠ¡** | MS MARCO (æ–‡æ¡£æ£€ç´¢) | MS MARCO |
| **å¾®è°ƒç±»å‹** | ä»»åŠ¡ä¸“é—¨å¾®è°ƒ | ä»»åŠ¡ä¸“é—¨å¾®è°ƒ |
| **é¦–æ¬¡æ¨ç†** | 300-800ms | 100-300ms |
| **åç»­æ¨ç†** | 50-150ms / 30 docs | **20-80ms** / 30 docs (3x) |
| **ååé‡** | ~100-300 docs/s | ~300-800 docs/s |

#### ä½¿ç”¨ç¯èŠ‚

#### 2.1 ç»“æœé‡æ’ (Reranking)

**å‘½ä»¤**: `qmd query "query" --rerank` (é»˜è®¤å¯ç”¨)

**è®¡ç®—è®¾å¤‡**: **CPU** (é»˜è®¤) æˆ– **GPU** (å¯é€‰)

**ä»£ç ä½ç½®**: `qmd/search/rerank.py` â†’ `rerank()`

**æµç¨‹**:
```python
# qmd/cli.py (lines 586-604)
def query(ctx_obj, query, collection, limit, rerank):
    """Hybrid search (BM25 + Vector) with optional reranking"""
    results = hybrid.search(query, collection, limit)
    
    if rerank:
        from qmd.search.rerank import LLMReranker
        reranker = LLMReranker()  # CPU æˆ– GPU
        
        # LLM é‡æ’åº (CPU æˆ– GPU)
        results = reranker.rerank(query, results, top_k=limit)
```

**è°ƒç”¨**:
```python
# qmd/search/rerank.py (lines 81-118)
def rerank(self, query: str, documents: List[Dict], top_k: int = 10):
    """Rerank documents using cross-encoder."""
    if not self.model:
        return documents[:top_k]
    
    # å‡†å¤‡ (æŸ¥è¯¢, æ–‡æ¡£) å¯¹
    pairs = [[query, doc.get("content", doc.get("title", ""))] for doc in documents]
    
    # æ¨ç† (CPU æˆ– GPU)
    with self._torch.no_grad():
        inputs = self._tokenizer(
            pairs,
            padding=True,
            truncation=True,
            return_tensors="pt",
            max_length=512
        )
        outputs = self._model(**inputs)  # CPU æˆ– GPU
        scores = outputs.logits.squeeze(-1)
    
    # æ·»åŠ åˆ†æ•°åˆ°æ–‡æ¡£
    for i, doc in enumerate(documents):
        doc["rerank_score"] = float(scores[i])
    
    # æŒ‰åˆ†æ•°æ’åº
    reranked = sorted(documents, key=lambda x: x.get("rerank_score", 0), reverse=True)
    return reranked[:top_k]
```

**æ€§èƒ½**:
- **CPU**: æ¨¡å‹åŠ è½½ 300-800ms (é¦–æ¬¡)ï¼Œ10 æ–‡æ¡£ 50-150msï¼Œ30 æ–‡æ¡£ 100-300ms
- **GPU**: æ¨¡å‹åŠ è½½ 100-300ms (é¦–æ¬¡)ï¼Œ10 æ–‡æ¡£ **20-50ms** (3x)ï¼Œ30 æ–‡æ¡£ **50-100ms** (3x)

**å½±å“**:
- âœ… æé«˜ç²¾åº¦ï¼š+15-20%
- âš ï¸ å¢åŠ å»¶è¿Ÿï¼šCPU +50-300msï¼ŒGPU +20-100ms
- ğŸ’¡ **å»ºè®®**: å°ç»“æœé›† (â‰¤10) å¯ç”¨

---

### 3. Expansion æ¨¡å‹ï¼š`Qwen3-0.5B-Instruct`

#### åŸºæœ¬ä¿¡æ¯

| å±æ€§ | å€¼ |
|------|------|
| **å®Œæ•´åç§°** | `Qwen/Qwen3-0.5B-Instruct` |
| **åŸºç¡€æ¨¡å‹** | Qwen3 0.5B (é˜¿é‡Œå·´å·´) |
| **ç±»å‹** | Causal LM (Generation) |
| **æ–‡ä»¶å¤§å°** | ~1.0GB |
| **è¾“å‡º** | 2-3 æŸ¥è¯¢å˜ä½“ |
| **ä¸Šä¸‹æ–‡çª—å£** | 32768 tokens |
| **æ ¼å¼** | PyTorch (FP16) |
| **é»˜è®¤è®¾å¤‡** | **CPU** (transformers) |
| **GPU æ”¯æŒ** | âœ… æ˜¯ (éœ€ CUDA) |
| **HuggingFace** | https://huggingface.co/Qwen/Qwen3-0.5B-Instruct |

#### æ€§èƒ½ç‰¹ç‚¹

| æŒ‡æ ‡ | CPU | GPU (CUDA) |
|------|-----|-----------|
| **æ¨¡å‹ç±»å‹** | Instruction-tuned | Instruction-tuned |
| **é¦–æ¬¡æ¨ç†** | 800-2000ms | 300-800ms |
| **åç»­æ¨ç†** | 400-1200ms / expansion | **150-500ms** / expansion (3x) |
| **ç”Ÿæˆé€Ÿåº¦** | ~2-5 tokens/50 max_tokens | ~5-15 tokens/50 max_tokens |
| **è´¨é‡** | ä¼˜ç§€çš„æŸ¥è¯¢ç†è§£ | ä¼˜ç§€çš„æŸ¥è¯¢ç†è§£ |

#### ä½¿ç”¨ç¯èŠ‚

#### 3.1 æŸ¥è¯¢æ‰©å±• (Query Expansion)

**å‘½ä»¤**: `qmd query "query" --rerank` (é»˜è®¤å¯ç”¨)

**è®¡ç®—è®¾å¤‡**: **CPU** (é»˜è®¤) æˆ– **GPU** (å¯é€‰)

**ä»£ç ä½ç½®**: `qmd/search/rerank.py` â†’ `expand_query()`

**æµç¨‹**:
```python
# qmd/search/rerank.py (lines 47-79)
def expand_query(self, query: str) -> List[str]:
    """Expand query into 2-3 variants using local Qwen3."""
    if not self.expansion_model:
        return [query]
    
    try:
        prompt = f"""Given the following search query, generate 2 alternative search queries that capture the same intent but use different wording or synonyms. Return only the variants, one per line.

Query: {query}
"""
        
        inputs = self._expansion_tokenizer(prompt, return_tensors="pt")
        
        # æ¨ç† (CPU æˆ– GPU)
        with self._expansion_model._torch.no_grad():
            outputs = self._expansion_model.generate(
                **inputs,
                max_new_tokens=50,
                temperature=0.7,
                do_sample=True,
                pad_token_id=self._expansion_tokenizer.eos_token_id
            )  # CPU æˆ– GPU
        
        response = self._expansion_tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # è§£æå˜ä½“ (åœ¨ "Query:" è¡Œä¹‹å)
        if "Query:" in response:
            response = response.split("Query:")[-1].strip()
        
        variants = [v.strip() for v in response.split("\n") if v.strip()]
        return [query] + variants[:2]
    except Exception as e:
        print(f"Query expansion error: {e}")
        return [query]
```

**æ€§èƒ½**:
- **CPU**: æ¨¡å‹åŠ è½½ 800-2000ms (é¦–æ¬¡)ï¼Œç”Ÿæˆ 2-3 å˜ä½“ 400-1200ms
- **GPU**: æ¨¡å‹åŠ è½½ 300-800ms (é¦–æ¬¡)ï¼Œç”Ÿæˆ 2-3 å˜ä½“ **150-500ms** (3x)

**å½±å“**:
- âœ… æé«˜å¬å›ï¼š+10-15%
- âš ï¸ å¢åŠ å»¶è¿Ÿï¼šCPU +400-1200msï¼ŒGPU +150-500ms
- ğŸ’¡ **å»ºè®®**: å¤æ‚æŸ¥è¯¢å¯ç”¨

---

## äºŒã€æœç´¢æµç¨‹ä¸æ¨¡å‹è°ƒç”¨

### 2.1 BM25 æœç´¢ (æ— æ¨¡å‹)

**å‘½ä»¤**: `qmd search "query"`

**è®¡ç®—è®¾å¤‡**: **CPU** (SQLite FTS5)

**ä»£ç ä½ç½®**: `qmd/search/fts.py`

**æµç¨‹**:
```python
# qmd/search/fts.py
SELECT
    d.id,
    d.path,
    d.title,
    snippet(c.doc, -2, '[b]', '[/b]', 30) as snippet,
    bm25(c.doc) AS score,
    c.doc as content
FROM documents_fts c
JOIN documents d ON d.id = c.docid
WHERE documents_fts MATCH ?
ORDER BY score
LIMIT ?
```

**ç‰¹ç‚¹**:
- âœ… SQLite FTS5 å†…ç½®
- âœ… æ— å¤–éƒ¨æ¨¡å‹
- âœ… æé€Ÿ (**CPU**: 40-50ms)
- âœ… å…³é”®è¯é«˜äº®
- ğŸ”§ **çº¯ CPU** (æ—  GPU æ”¯æŒ)

### 2.2 å‘é‡æœç´¢ (å•æ¨¡å‹ï¼šbge-small)

**å‘½ä»¤**: `qmd vsearch "query"`

**è®¡ç®—è®¾å¤‡**: **CPU** (é»˜è®¤) æˆ– **GPU** (å¯é€‰)

**ä»£ç ä½ç½®**: `qmd/search/vector.py`

**æµç¨‹**:
```python
# 1. æŸ¥è¯¢åµŒå…¥ (bge-small, CPU/GPU)
query_embedding = llm.embed_query(query)  # CPU: 30-50ms, GPU: 10-20ms

# 2. ChromaDB æœç´¢ (HNSW, CPU)
results = collection.query(
    query_embeddings=[query_embedding],
    n_results=5,
    include=["documents", "metadatas", "distances"]
)  # 50-200ms

# 3. ç»“æœæ ¼å¼åŒ– (CPU)
search_results = [...]
```

**æ¨¡å‹**:
- âœ… `bge-small-en-v1.5` (åµŒå…¥)
- æ€§èƒ½ (CPU): **80-250ms**
- æ€§èƒ½ (GPU): **60-220ms**

### 2.3 æ··æˆæœç´¢ (å¤šæ¨¡å‹ï¼šbge-small + ms-marco + Qwen3)

**å‘½ä»¤**: `qmd query "query"` (é»˜è®¤å¯ç”¨æ‰€æœ‰æ¨¡å‹)

**è®¡ç®—è®¾å¤‡**: **CPU** (é»˜è®¤) æˆ– **GPU** (å¯é€‰)

**ä»£ç ä½ç½®**: `qmd/search/hybrid.py` + `qmd/search/rerank.py`

**æµç¨‹**:
```python
# 1. æŸ¥è¯¢æ‰©å±• (Qwen3-0.5B-Instruct, CPU/GPU)
expanded_queries = reranker.expand_query(query)  # CPU: 400-1200ms, GPU: 150-500ms
# ["query", "variant 1", "variant 2"]

# 2. BM25 æœç´¢ (FTS5, CPU)
for q in expanded_queries:
    fts_results = fts.search(q)  # CPU: 40-50ms each

# 3. å‘é‡æœç´¢ (bge-small, CPU/GPU)
for q in expanded_queries:
    vector_results = vector.search(q)  # CPU: 80-250ms each, GPU: 60-220ms each

# 4. RRF èåˆ (CPU)
scores[doc_id] = sum(1 / (60 + rank) for rank in ranks)  # 10-20ms

# 5. LLM é‡æ’ (ms-marco-MiniLM-L-6-v2, CPU/GPU)
results = reranker.rerank(query, results, top_k=10)  # CPU: 50-300ms, GPU: 20-100ms

# 6. è¿”å›æœ€ç»ˆç»“æœ
return results[:10]
```

**æ¨¡å‹**:
- âœ… `Qwen3-0.5B-Instruct` (æ‰©å±•)
- âœ… `bge-small-en-v1.5` (åµŒå…¥)
- âœ… `ms-marco-MiniLM-L-6-v2` (é‡æ’)

**æ€§èƒ½**:
- **CPU (é¦–æ¬¡)**: 500-2000ms (å«æ‰©å±•)
- **CPU (åç»­)**: 200-800ms (æ— æ‰©å±•)
- **GPU (é¦–æ¬¡)**: 300-800ms (å«æ‰©å±•)
- **GPU (åç»­)**: 100-400ms (æ— æ‰©å±•)

---

## ä¸‰ã€å®Œæ•´æµç¨‹å›¾ (å«è®¾å¤‡æ ‡æ³¨)

### 3.1 CPU é…ç½® (é»˜è®¤)

```
ç”¨æˆ·æŸ¥è¯¢ "how to authenticate"
    â”‚
    â”œâ”€ [BM25] qmd search â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                           â”‚
    â”‚                                         SQLite FTS5 (CPU)
    â”‚                                           â”‚
    â”‚                                  40-50ms (çº¯ CPU)
    â”‚                                           â”‚
    â”‚                                   å…³é”®è¯åŒ¹é…ç»“æœ
    â”‚
    â”œâ”€ [Vector] qmd vsearch â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                          â”‚                  â”‚
    â”‚                          â”‚         bge-small åµŒå…¥
    â”‚                          â”‚         (fastembed, CPU)
    â”‚                          â”‚         30-50ms
    â”‚                          â”‚                  â”‚
    â”‚                          â”‚         ChromaDB æœç´¢
    â”‚                          â”‚         (HNSW, CPU)
    â”‚                          â”‚         50-200ms
    â”‚                          â”‚                  â”‚
    â”‚                          80-250ms (è¯­ä¹‰ç»“æœ)
    â”‚
    â””â”€ [Hybrid] qmd query â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚                        â”‚
                       â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
                       â”‚         â”‚                    â”‚
                       â”‚  Qwen3 æ‰©å±•          â”‚
                       â”‚  (transformers, CPU)    â”‚
                       â”‚  400-1200ms            â”‚
                       â”‚         â”‚                    â”‚
                       â”‚  2-3 å˜ä½“               â”‚
                       â”‚         â”‚                    â”‚
                       â”‚         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                       â”‚         â”‚  FTS (x3, CPU)      â”‚
                       â”‚         â”‚  40-50ms each       â”‚
                       â”‚         â”‚                    â”‚
                       â”‚         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                       â”‚         â”‚                    â”‚
                       â”‚         â”‚  bge-small åµŒå…¥ (CPU)â”‚
                       â”‚         â”‚  30-50ms each        â”‚
                       â”‚         â”‚                    â”‚
                       â”‚         â”‚  ChromaDB (CPU)      â”‚
                       â”‚         â”‚  50-200ms each      â”‚
                       â”‚         â”‚                    â”‚
                       â”‚         â””â”€ RRF èåˆ (CPU)   â”‚
                       â”‚              10-20ms      â”‚
                       â”‚         â”‚                    â”‚
                       â”‚         â””â”€ LLM é‡æ’           â”‚
                       â”‚         ms-marco (CPU)       â”‚
                       â”‚         50-300ms            â”‚
                       â”‚                             â”‚
                       â”‚         200-800ms (æœ€ä¼˜ç»“æœ)  â”‚
                       â”‚                             â”‚
                   çº¯ CPU è®¡ç®—                      â”‚
```

**æ€»æ€§èƒ½ (CPU)**:
- BM25: 40-50ms
- Vector: 80-250ms
- Hybrid (å«æ‰©å±•): 500-2000ms (é¦–æ¬¡)ï¼Œ200-800ms (åç»­)
- Hybrid (æ— æ‰©å±•): 200-800ms

### 3.2 GPU é…ç½® (å¯é€‰ï¼Œéœ€ CUDA)

```
ç”¨æˆ·æŸ¥è¯¢ "how to authenticate"
    â”‚
    â”œâ”€ [BM25] qmd search â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                           â”‚
    â”‚                                         SQLite FTS5 (CPU)
    â”‚                                           â”‚
    â”‚                                  40-50ms (çº¯ CPU)
    â”‚                                           â”‚
    â”‚                                   å…³é”®è¯åŒ¹é…ç»“æœ
    â”‚
    â”œâ”€ [Vector] qmd vsearch â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                          â”‚                  â”‚
    â”‚                          â”‚         bge-small åµŒå…¥
    â”‚                          â”‚         (fastembed, GPU)
    â”‚                          â”‚         10-20ms âš¡
    â”‚                          â”‚                  â”‚
    â”‚                          â”‚         ChromaDB æœç´¢
    â”‚                          â”‚         (HNSW, CPU)
    â”‚                          â”‚         50-200ms
    â”‚                          â”‚                  â”‚
    â”‚                          60-220ms (è¯­ä¹‰ç»“æœ) âš¡
    â”‚
    â””â”€ [Hybrid] qmd query â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚                        â”‚
                       â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
                       â”‚         â”‚                    â”‚
                       â”‚  Qwen3 æ‰©å±•          â”‚
                       â”‚  (transformers, GPU)    â”‚
                       â”‚  150-500ms âš¡          â”‚
                       â”‚         â”‚                    â”‚
                       â”‚  2-3 å˜ä½“               â”‚
                       â”‚         â”‚                    â”‚
                       â”‚         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                       â”‚         â”‚  FTS (x3, CPU)      â”‚
                       â”‚         â”‚  40-50ms each       â”‚
                       â”‚         â”‚                    â”‚
                       â”‚         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                       â”‚         â”‚                    â”‚
                       â”‚         â”‚  bge-small åµŒå…¥ (GPU)â”‚
                       â”‚         â”‚  10-20ms each âš¡   â”‚
                       â”‚         â”‚                    â”‚
                       â”‚         â”‚  ChromaDB (CPU)      â”‚
                       â”‚         â”‚  50-200ms each      â”‚
                       â”‚         â”‚                    â”‚
                       â”‚         â””â”€ RRF èåˆ (CPU)   â”‚
                       â”‚              10-20ms      â”‚
                       â”‚         â”‚                    â”‚
                       â”‚         â””â”€ LLM é‡æ’           â”‚
                       â”‚         ms-marco (GPU)       â”‚
                       â”‚         20-100ms âš¡         â”‚
                       â”‚                             â”‚
                       â”‚         100-400ms (æœ€ä¼˜ç»“æœ)âš¡â”‚
                       â”‚                             â”‚
               GPU åŠ é€Ÿï¼Œ3x æå‡                â”‚
```

**æ€»æ€§èƒ½ (GPU)**:
- BM25: 40-50ms (æ—  GPU)
- Vector: 60-220ms âš¡
- Hybrid (å«æ‰©å±•): 300-800ms (é¦–æ¬¡) âš¡ï¼Œ100-400ms (åç»­) âš¡
- Hybrid (æ— æ‰©å±•): 100-400ms âš¡

**æ³¨**: âš¡ = GPU åŠ é€Ÿç¯èŠ‚ (3x æå‡)

---

## å››ã€è®¾å¤‡é…ç½®è¯¦è§£

### 4.1 é»˜è®¤é…ç½® (çº¯ CPU)

**ç‰¹ç‚¹**:
- âœ… æ— éœ€é¢å¤–å®‰è£…
- âœ… é€‚ç”¨äºæ‰€æœ‰ç¡¬ä»¶
- âš ï¸ æ€§èƒ½ä¸­ç­‰

**é…ç½®**:
```bash
# å®‰è£…
pip install .

# ä½¿ç”¨
qmd query "how to authenticate"
```

**è®¾å¤‡åˆ†é…**:
| ç¯èŠ‚ | è®¾å¤‡ | æ¨¡å‹/å¼•æ“ |
|------|------|-----------|
| BM25 æœç´¢ | CPU | SQLite FTS5 |
| å‘é‡åµŒå…¥ | CPU | bge-small (fastembed) |
| å‘é‡æœç´¢ | CPU | ChromaDB HNSW |
| æŸ¥è¯¢æ‰©å±• | CPU | Qwen3 (transformers) |
| ç»“æœé‡æ’ | CPU | ms-marco (transformers) |
| RRF èåˆ | CPU | Python è®¡ç®— |

### 4.2 GPU é…ç½® (å¯é€‰ï¼Œéœ€ NVIDIA)

**ç‰¹ç‚¹**:
- âš¡ï¸ éœ€è¦ NVIDIA GPU + CUDA
- âœ… 3x æ€§èƒ½æå‡
- âš ï¸ éœ€è¦é¢å¤–å®‰è£…

**å®‰è£…**:
```bash
# 1. æ£€æŸ¥ GPU
nvidia-smi  # åº”æ˜¾ç¤º GPU ä¿¡æ¯

# 2. å®‰è£… CUDA ç‰ˆæœ¬ torch
pip uninstall torch  # å…ˆå¸è½½ CPU ç‰ˆæœ¬
pip install torch --index-url https://download.pytorch.org/whl/cu121

# 3. éªŒè¯
python -c "import torch; print(torch.cuda.is_available())"  # åº”è¾“å‡º True
```

**é…ç½®**:
```python
# qmd/llm/engine.py (ä¿®æ”¹ä»¥æ”¯æŒ GPU)
import torch

class LLMEngine:
    def __init__(self, device: str = "auto"):
        if device == "auto":
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device
        
        # fastembed è‡ªåŠ¨æ£€æµ‹ GPU
        self._model = TextEmbedding(
            model_name=self.model_name,
            device=self.device  # "cuda" or "cpu"
        )
    
    def embed_texts(self, texts: List[str]) -> List[List[float]]:
        # æ¨¡å‹æ¨ç† (GPU æˆ– CPU)
        embeddings = list(self._model.embed(texts))
        return [emb.tolist() for emb in embeddings]
```

**ä½¿ç”¨**:
```bash
# è‡ªåŠ¨æ£€æµ‹ GPU
qmd query "how to authenticate"

# æˆ–æ‰‹åŠ¨æŒ‡å®š (æœªå®ç°)
QMD_DEVICE=cuda qmd query "how to authenticate"
```

**è®¾å¤‡åˆ†é…**:
| ç¯èŠ‚ | è®¾å¤‡ | æ¨¡å‹/å¼•æ“ |
|------|------|-----------|
| BM25 æœç´¢ | CPU | SQLite FTS5 (æ—  GPU) |
| å‘é‡åµŒå…¥ | **GPU** âš¡ | bge-small (fastembed) |
| å‘é‡æœç´¢ | CPU | ChromaDB HNSW (æ—  GPU) |
| æŸ¥è¯¢æ‰©å±• | **GPU** âš¡ | Qwen3 (transformers) |
| ç»“æœé‡æ’ | **GPU** âš¡ | ms-marco (transformers) |
| RRF èåˆ | CPU | Python è®¡ç®— |

**æ€§èƒ½æå‡**:
- å‘é‡åµŒå…¥: 30-50ms â†’ **10-20ms** (3x) âš¡
- æŸ¥è¯¢æ‰©å±•: 400-1200ms â†’ **150-500ms** (3x) âš¡
- ç»“æœé‡æ’: 50-300ms â†’ **20-100ms** (3x) âš¡
- æ€»è®¡: 200-800ms â†’ **100-400ms** (2x) âš¡

---

## äº”ã€æ¨¡å‹åŠ è½½ä¸ç¼“å­˜

### 5.1 é¦–æ¬¡ä½¿ç”¨ (è‡ªåŠ¨ä¸‹è½½)

**è§¦å‘**: ä»»ä½•éœ€è¦æ¨¡å‹çš„å‘½ä»¤é¦–æ¬¡è¿è¡Œ

**è®¾å¤‡**: **CPU** (é»˜è®¤) æˆ– **GPU** (é¦–æ¬¡åŠ è½½)

**æµç¨‹**:
```python
# qmd/llm/engine.py (lines 21-30)
def _ensure_model(self):
    """Load model if not already loaded."""
    if self._model is not None:
        return
    
    # fastembed è‡ªåŠ¨ä» HuggingFace ä¸‹è½½
    self._model = TextEmbedding(
        model_name="BAAI/bge-small-en-v1.5",
        cache_dir=self.cache_dir,  # ~/.cache/huggingface/
        threads=self.threads
    )  # CPU æˆ– GPU
```

**ä¸‹è½½ä½ç½®**:
- Linux/macOS: `~/.cache/huggingface/hub/`
- Windows: `C:\Users\<username>\.cache\huggingface\hub\`

**æç¤º**:
```bash
$ qmd vsearch "test query"
Downloading model...
[========                               ] 15% (30MB/130MB)
```

### 5.2 æ¨¡å‹ç¼“å­˜ (åç»­ä½¿ç”¨)

**è§¦å‘**: ç¬¬äºŒæ¬¡åŠä»¥åä½¿ç”¨

**è®¾å¤‡**: **CPU** (é»˜è®¤) æˆ– **GPU** (å·²åŠ è½½æ¨¡å‹)

**æµç¨‹**:
```python
# qmd/llm/engine.py (lines 21-24)
def _ensure_model(self):
    """Load model if not already loaded."""
    if self._model is not None:  # âœ… å·²åŠ è½½ï¼Œç›´æ¥è¿”å›
        return
    # ... åŠ è½½æ¨¡å‹
```

**æ€§èƒ½æå‡**:
- é¦–æ¬¡ (CPU): 500-2000ms
- é¦–æ¬¡ (GPU): 200-500ms
- åç»­ (CPU): 30-150ms
- åç»­ (GPU): **10-50ms** âš¡
- **æå‡**: ~10-20x

### 5.3 å†…å­˜é©»ç•™ (å¸¸é©»æ¨¡å¼)

**å½“å‰å®ç°**: æ¨¡å‹åœ¨ Python è¿›ç¨‹ç”Ÿå‘½å‘¨æœŸå†…é©»ç•™å†…å­˜

**å½±å“**:
- âœ… å¿«é€Ÿæ¨ç† (CPU: 30-150ms, GPU: 10-50ms)
- âš ï¸ å†…å­˜å ç”¨ (CPU: ~2.5GB, GPU: ~1.5GB + VRAM)
- ğŸ’¡ **å»ºè®®**: é¢‘ç¹ä½¿ç”¨åœºæ™¯ä¸‹ä¿æŒå¸¸é©»

---

## å…­ã€æ¨¡å‹é€‰æ‹©ä¸å¯¹æ¯”

### 6.1 è®¾è®¡æ–‡æ¡£ vs å®é™…å®ç°

| ç»„ä»¶ | è®¾è®¡æ–‡æ¡£ (06-models.md) | å®é™…å®ç° | å˜æ›´åŸå›  |
|------|----------------------|---------|---------|
| **Embedding** | `embeddingemma-2b` | `bge-small-en-v1.5` | âœ… **è´¨é‡æå‡** |
| **Reranking** | `qwen3-reranker` | `ms-marco-MiniLM-L-6-v2` | âœ… **ä»»åŠ¡å¾®è°ƒ** |
| **Expansion** | `qwen3-query-expansion` | `Qwen3-0.5B-Instruct` | âœ… **æœ¬åœ°è¿è¡Œ** |

### 6.2 å¤§å°å¯¹æ¯”

| æ¨¡å‹ | è®¾è®¡æ–‡æ¡£ | å®é™…å®ç° | å·®å¼‚ |
|------|---------|---------|------|
| **Embedding** | ~300MB | ~130MB | -170MB âœ… |
| **Reranking** | ~640MB | ~110MB | -530MB âœ… |
| **Expansion** | ~1.1GB | ~1.0GB | -100MB âœ… |
| **æ€»è®¡** | ~2.04GB | ~1.24GB | -38% âœ… |

### 6.3 æ€§èƒ½å¯¹æ¯” (CPU vs GPU)

| æŒ‡æ ‡ | è®¾è®¡æ–‡æ¡£ | å®é™… CPU | å®é™… GPU | å·®å¼‚ |
|------|---------|----------|----------|------|
| **Embedding** | 50-200ms | 30-150ms | **10-50ms** âš¡ | -100ms âœ… |
| **Reranking** | 100-300ms | 50-150ms | **20-80ms** âš¡ | -70ms âœ… |
| **Expansion** | 500-1500ms | 400-1200ms | **150-500ms** âš¡ | -700ms âœ… |

---

## ä¸ƒã€æ¨¡å‹ä¼˜åŒ–å»ºè®®

### 7.1 é™ä½å†…å­˜å ç”¨

#### é€‰é¡¹ 1: ä½¿ç”¨é‡åŒ–æ¨¡å‹ (å¯é€‰)

```python
# å½“å‰ï¼šFP16/FP32
model = AutoModel.from_pretrained("BAAI/bge-small-en-v1.5")

# ä¼˜åŒ–ï¼š8-bit é‡åŒ–
from transformers import BitsAndBytesConfig
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True  # 8-bit é‡åŒ–
)
model = AutoModel.from_pretrained(
    "BAAI/bge-small-en-v1.5",
    quantization_config=quantization_config
)
```

**æ•ˆæœ**:
- å†…å­˜: ~2.5GB â†’ ~2.0GB (CPU)ï¼Œ~1.5GB â†’ ~1.2GB (GPU)
- è´¨é‡: è½»å¾®ä¸‹é™ (<2%)

#### é€‰é¡¹ 2: å‡å°‘ä¸Šä¸‹æ–‡çª—å£

```python
# å½“å‰ï¼šé»˜è®¤ä¸Šä¸‹æ–‡
inputs = tokenizer(texts, max_length=512)

# ä¼˜åŒ–ï¼šå‡ä¸Šä¸‹æ–‡
inputs = tokenizer(texts, max_length=256)
```

**æ•ˆæœ**:
- å†…å­˜: ~2.5GB â†’ ~2.2GB (CPU)ï¼Œ-12%
- è´¨é‡: å¯¹çŸ­æ–‡æ¡£å½±å“å°

#### é€‰é¡¹ 3: åˆ†æ‰¹å¤„ç†

```python
# å½“å‰ï¼šå¤§æ‰¹é‡
embeddings = model.embed(texts)  # 1000 æ–‡æ¡£

# ä¼˜åŒ–ï¼šå°æ‰¹é‡
batch_size = 100
for i in range(0, len(texts), batch_size):
    batch = texts[i:i+batch_size]
    embeddings.extend(model.embed(batch))
```

**æ•ˆæœ**:
- å†…å­˜: ~500MB â†’ ~200MB (CPU)ï¼Œ-60%
- é€Ÿåº¦: ç¨æ…¢ (~10%)

### 7.2 æé«˜æ¨ç†é€Ÿåº¦

#### é€‰é¡¹ 1: GPU åŠ é€Ÿ (æ¨è)

```bash
# qmd/llm/engine.py
import torch

# æ£€æµ‹ GPU
if torch.cuda.is_available():
    device = "cuda"
else:
    device = "cpu"

# ç§»åŠ¨æ¨¡å‹åˆ° GPU
model.to(device)
embeddings = model.embed(texts)
```

**æ•ˆæœ**:
- é€Ÿåº¦: 2-5x æå‡ âš¡
- éœ€è¦: NVIDIA GPU + CUDA

#### é€‰é¡¹ 2: æ‰¹å¤„ç†ä¼˜åŒ–

```python
# å½“å‰ï¼šå•æ¡å¤„ç†
for text in texts:
    emb = model.embed(text)

# ä¼˜åŒ–ï¼šæ‰¹é‡å¤„ç†
embeddings = model.embed(texts)  # è‡ªåŠ¨æ‰¹é‡
```

**æ•ˆæœ**:
- é€Ÿåº¦: 3-5x æå‡
- å†…å­˜: ç•¥å¢ (~100MB)

---

## å…«ã€æ¨¡å‹æ€»ç»“

### 8.1 å®Œæ•´æ¸…å•

| # | æ¨¡å‹åç§° | HuggingFace | å¤§å° | ç”¨é€” | é»˜è®¤è®¾å¤‡ | GPU æ”¯æŒ |
|---|---------|-----------|------|------|---------|---------|
| **1** | `bge-small-en-v1.5` | [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) | 130MB | å‘é‡åµŒå…¥ | **CPU** | âœ… å¯é€‰ |
| **2** | `ms-marco-MiniLM-L-6-v2` | [cross-encoder/ms-marco-MiniLM-L-6-v2](https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-6-v2) | 110MB | ç»“æœé‡æ’ | **CPU** | âœ… å¯é€‰ |
| **3** | `Qwen3-0.5B-Instruct` | [Qwen/Qwen3-0.5B-Instruct](https://huggingface.co/Qwen/Qwen3-0.5B-Instruct) | 1.0GB | æŸ¥è¯¢æ‰©å±• | **CPU** | âœ… å¯é€‰ |
| **æ€»è®¡** | - | - | **1.24GB** | **CPU** | - |

### 8.2 åº”ç”¨ç¯èŠ‚æ€»ç»“

| æœç´¢ç±»å‹ | ä½¿ç”¨æ¨¡å‹ | è°ƒç”¨å‘½ä»¤ | è®¾å¤‡ | æ€§èƒ½ (CPU) | æ€§èƒ½ (GPU) |
|---------|---------|----------|------|----------|----------|
| **BM25** | æ—  (SQLite FTS5) | `qmd search` | CPU | 40-50ms | 40-50ms |
| **Vector** | bge-small | `qmd vsearch` | CPU/GPU | 80-250ms | 60-220ms âš¡ |
| **Hybrid** | bge-small + ms-marco + Qwen3 | `qmd query` | CPU/GPU | 200-800ms | 100-400ms âš¡ |
| **Embed** | bge-small | `qmd embed` | CPU/GPU | ~1s/100 docs | ~300ms/100 docs âš¡ |

### 8.3 ä¼˜åŒ–å»ºè®®

| åœºæ™¯ | å»ºè®® | åŸå›  |
|------|------|------|
| **è½»é‡ä½¿ç”¨** | ä¸»è¦ç”¨ `qmd search` | æ— æ¨¡å‹ï¼Œæå¿« |
| **å¸¸è§„ä½¿ç”¨** | ç”¨ `qmd vsearch` | å•æ¨¡å‹ï¼Œå¹³è¡¡ |
| **é«˜è´¨é‡** | ç”¨ `qmd query` | å¤šæ¨¡å‹ï¼Œæœ€ä¼˜ |
| **èµ„æºå—é™** | ç¦ç”¨é‡æ’ `qmd query --no-rerank` | å°‘ç”¨ 2 æ¨¡å‹ |
| **GPU å¯ç”¨** | å¯ç”¨ GPU åŠ é€Ÿ | 3x æ€§èƒ½æå‡ âš¡ |

---

## ä¹ã€å¸¸è§é—®é¢˜

### Q1: æ¨¡å‹ä¼šè‡ªåŠ¨ä¸‹è½½å—ï¼Ÿ

**A**: âœ… æ˜¯çš„ï¼Œé¦–æ¬¡ä½¿ç”¨æ—¶è‡ªåŠ¨ä» HuggingFace ä¸‹è½½ã€‚

```bash
$ qmd vsearch "test query"
Downloading bge-small-en-v1.5...
[=========                             ] 100%
```

### Q2: æ¨¡å‹ä¸‹è½½åˆ°å“ªäº†ï¼Ÿ

**A**: é»˜è®¤ä¸‹è½½åˆ° `~/.cache/huggingface/hub/`

- Linux/macOS: `/home/user/.cache/huggingface/hub/`
- Windows: `C:\Users\<username>\.cache\huggingface\hub\`

### Q3: å¯ä»¥ç¦»çº¿ä½¿ç”¨å—ï¼Ÿ

**A**: âœ… å¯ä»¥ï¼Œæ¨¡å‹ä¸‹è½½åå®Œå…¨ç¦»çº¿è¿è¡Œã€‚

### Q4: å¦‚ä½•æ›´æ¢æ¨¡å‹ï¼Ÿ

**A**: ä¿®æ”¹ `qmd/llm/engine.py` æˆ– `qmd/search/rerank.py`:

```python
# æ›´æ¢åµŒå…¥æ¨¡å‹
model_name = "BAAI/bge-base-en-v1.5"  # æ›´å¤§ä½†æ›´å‡†

# æ›´æ¢é‡æ’æ¨¡å‹
model_name = "BAAI/bge-reranker-base"  # SOTA
```

### Q5: å¦‚ä½•å¯ç”¨ GPU åŠ é€Ÿï¼Ÿ

**A**: éœ€è¦ä¸‰æ­¥ï¼š

1. **æ£€æŸ¥ GPU**:
   ```bash
   nvidia-smi  # åº”æ˜¾ç¤º GPU ä¿¡æ¯
   ```

2. **å®‰è£… CUDA ç‰ˆæœ¬ torch**:
   ```bash
   pip uninstall torch  # å…ˆå¸è½½ CPU ç‰ˆæœ¬
   pip install torch --index-url https://download.pytorch.org/whl/cu121
   ```

3. **éªŒè¯**:
   ```bash
   python -c "import torch; print(torch.cuda.is_available())"  # åº”è¾“å‡º True
   ```

**æ•ˆæœ**: 3x æ€§èƒ½æå‡ âš¡

### Q6: å¦‚ä½•å‡å°‘å†…å­˜å ç”¨ï¼Ÿ

**A**: å‚è§"ä¸ƒã€æ¨¡å‹ä¼˜åŒ–å»ºè®®"ï¼š
1. ä½¿ç”¨é‡åŒ–æ¨¡å‹ (-20%)
2. å‡å°‘ä¸Šä¸‹æ–‡çª—å£ (-12%)
3. å°æ‰¹é‡å¤„ç† (-60%)

### Q7: GPU åŠ é€Ÿæ”¯æŒå“ªäº›ç¯èŠ‚ï¼Ÿ

**A**: 
- âœ… å‘é‡åµŒå…¥ (bge-small) âš¡
- âœ… æŸ¥è¯¢æ‰©å±• (Qwen3) âš¡
- âœ… ç»“æœé‡æ’ (ms-marco) âš¡
- âŒ BM25 æœç´¢ (ä»… CPU)
- âŒ ChromaDB æœç´¢ (ä»… CPU)

**æ³¨**: âš¡ = GPU åŠ é€Ÿç¯èŠ‚ (3x æå‡)

---

**æ–‡æ¡£ç»“æŸ**
